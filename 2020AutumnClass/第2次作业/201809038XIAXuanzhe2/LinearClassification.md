# 线性分类

## 概念

**神经网络的一个重要功能就是分类，现实世界中的分类任务复杂多样，但万变不离其宗，我们都可以用同一种模式的神经网络来处理**

我们将利用学到的二分类知识，实现逻辑与门、与非门，或门，或非门。

做二分类时，我们一般用Sigmoid函数做分类函数，那么和Sigmoid函数长得特别像的双曲正切函数能不能做分类函数呢？我们将会探索这件事情，从而对分类函数、损失函数、样本标签有更深的理解。

然后我们将进入线性多分类的学习。多分类时，可以一对一、一对多、多对多

Softmax函数是多分类问题的分类函数，通过对它的分析，我们学习多分类的原理、实现、以及可视化结果，从而理解神经网络的工作方式。

我们本章要学习的路径是：回归问题->逻辑回归问题->线性逻辑回归即分类问题->线性二分类问题。

|线性二分类|非线性二分类|
|---|---|
|![](img/img6.jpg)|![](img/img7.jpg)|

## 分类函数

### 概念

对线性和非线性二分类都适用。
对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。
在二分类任务中，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。

Logistic函数公式

$$Logistic(z) = \frac{1}{1 + e^{-z}}$$

以下记 $a=Logistic(z)$。

导数

$$Logistic'(z) = a(1 - a)$$

具体求导过程可以参考8.1节。

输入值域

$$(-\infty, \infty)$$

输出值域

$$(0,1)$$

函数图像

![](img/img8.jpg)

## 用神经网络实现线性二分类

### 定义神经网络结构

根据前面的猜测，看来我们只需要一个二入一出的神经元就可以搞定。这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络，见图6-3。

![](img/img9.jpg)

完成二分类任务的神经元结构

与上一章的网络结构图的区别是，这次我们在神经元输出时使用了分类函数，所以输出为 $A$，而不是以往直接输出的 $Z$。

#### 输入层

输入经度 $x_1$ 和纬度 $x_2$ 两个特征：

$$
X=\begin{pmatrix}
x_{1} & x_{2}
\end{pmatrix}
$$

#### 权重矩阵

输入是2个特征，输出一个数，则 $W$ 的尺寸就是 $2\times 1$：

$$
W=\begin{pmatrix}
w_{1} \\\\ w_{2}
\end{pmatrix}
$$

$B$ 的尺寸是 $1\times 1$，行数永远是1，列数永远和 $W$ 一样。

$$
B=\begin{pmatrix}
b
\end{pmatrix}
$$

#### 输出层

$$
\begin{aligned}    
z &= X \cdot W + B
=\begin{pmatrix}
    x_1 & x_2
\end{pmatrix}
\begin{pmatrix}
    w_1 \\\\ w_2
\end{pmatrix} + b \\\\
&=x_1 \cdot w_1 + x_2 \cdot w_2 + b 
\end{aligned}
\tag{1}
$$
$$a = Logistic(z) \tag{2}$$

#### 损失函数

二分类交叉熵损失函数：

$$
loss(W,B) = -[y\ln a+(1-y)\ln(1-a)] \tag{3}
$$

### 6.2.2 反向传播

我们在6.1节已经推导了 $loss$ 对 $z$ 的偏导数，结论为 $A-Y$。接下来，我们求 $loss$ 对 $W$ 的导数。本例中，$W$ 的形式是一个2行1列的向量，所以求 $W$ 的偏导时，要对向量求导：

$$
\frac{\partial loss}{\partial w}=
\begin{pmatrix}
    \frac{\partial loss}{\partial w_1} \\\\ 
    \frac{\partial loss}{\partial w_2}
\end{pmatrix}
$$
$$
=\begin{pmatrix}
 \frac{\partial loss}{\partial z}\frac{\partial z}{\partial w_1} \\\\
 \frac{\partial loss}{\partial z}\frac{\partial z}{\partial w_2}   
\end{pmatrix}
=\begin{pmatrix}
    (a-y)x_1 \\\\
    (a-y)x_2
\end{pmatrix}
$$
$$
=(x_1 \ x_2)^{\top} (a-y) \tag{4}
$$

上式中$x_1,x_2$是一个样本的两个特征值。如果是多样本的话，公式4将会变成其矩阵形式，以3个样本为例：

$$
\frac{\partial J(W,B)}{\partial W}=
\begin{pmatrix}
    x_{11} & x_{12} \\\\
    x_{21} & x_{22} \\\\
    x_{31} & x_{32} 
\end{pmatrix}^{\top}
\begin{pmatrix}
    a_1-y_1 \\\\
    a_2-y_2 \\\\
    a_3-y_3 
\end{pmatrix}
=X^{\top}(A-Y) \tag{5}
$$

## 线性二分类原理

#### 基本公式回顾

下面我们以单样本双特征值为例来说明神经网络的二分类过程，这是用代数方式来解释其工作原理。

1. 正向计算

$$
z = x_1 w_1+ x_2 w_2 + b  \tag{1}
$$

2. 分类计算

$$
a={1 \over 1 + e^{-z}} \tag{2}
$$

3. 损失函数计算

$$
loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3}
$$

用图6-5举例来说明计算过程。

![](img/img10.jpg)

图6-5 不正确的分类线试图分类红绿两色样本点

平面上有三个点，分成两类，绿色方块为正类，红色三角为负类。各个点的坐标为：$A(2,4)，B(2,1)，C(3,3)$。

#### 分类线为 $L_1$ 时

假设神经网络第一次使用 $L_1$ 做为分类线，此时：$w_1=-1,w_2=2,b=-2$，我们来计算一下三个点的情况。

$A$点：

$$
z_A = (-1)\times 2 + 2 \times 4 -2 = 4 > 0 \tag{正确}
$$

$B$点：

$$
z_B = (-1)\times 2 + 2 \times 1 -2 = -2 < 0 \tag{正确}
$$

$C$点：

$$
z_C = (-1)\times 3 + 2 \times 3 -2 = 1 > 0 \tag{错误}
$$

我们知道当 $z>0$ 时，$Logistic(z) > 0.5$ 为正例，反之为负例，所以我们只需要看三个点的 $z$ 值是大于0还是小于0就可以了，不用再计算 $Logistic$ 函数值。

其中，$A,B$点处于正确的分类区，而 $C$ 点处于错误的分类区。此时 $C$ 点的损失函数值为（注意 $C$ 的标签值 $y=0$）：

$$
a_C = Logistic(z_C) = 0.731
$$

$$
loss_Z = -(0 \cdot \ln(0.731) + 1 \cdot \ln(1-0.731))=1.313
$$

读者可能对 $1.313$ 这个值没有什么概念，是大还是小呢？我们不妨计算一下分类正确的 $A,B$ 点的坐标：

$$
loss_A = 0.018, \quad loss_B = 0.112
$$

可见，对于分类正确的 $A,B$ 点来说，其损失函数值比 $C$ 点要小很多，所以 $C$ 点的反向传播的力度就大。对比总结如表6-5。

表6-5 对比三个点在各个环节的计算值

|点|坐标值|$z$ 值|$a$ 值|$y$ 值|$loss$ 值|分类情况|
|---|---|---|---|---|---|---|
|A|(2,4)|4|0.982|1|0.018|正确|
|B|(2,1)|-2|0.119|0|0.112|正确|
|C|(3,3)|1|0.731|0|1.313|错误|

- 在正例情况 $y=1$ 时，$a$ 如果越靠近 $1$，表明分类越正确，此时损失值会越小。点 $A$ 就是这种情况：$a=0.982$，距离 $1$ 不远；$loss$ 值 $0.018$，很小；
- 在负例情况 $y=0$ 时，$a$ 如果越靠近 $0$，表明分类越正确，此时损失值会越小。点 $B$ 就是这种情况：$a=0.119$，距离 $0$ 不远；$loss$ 值 $0.112$，不算很大；
- 点 $C$ 是分类错误的情况，$a=0.731$，本应小于 $0.5$，实际上距离 $0$ 远，距离 $1$ 反而近，它的 $loss=1.313$，与其它两个点的相对值来看非常大，这样误差就大，反向传播的力度也大。

#### 分类线为 $L_2$ 时

我们假设经过反向传播后，神经网络把直线的位置调整到 $L_2$，以 $L_2$ 做为分类线，即 $w_1=-1,w_2=1,b=-1$，则三个点的 $z$ 值都会是符合其分类的：

$$
z_A = (-1)\times 2 + 1 \times 4 -1 = 1 > 0 \tag{正确}
$$

$$
z_B = (-1)\times 2 + 1 \times 1 -1 = -2 < 0 \tag{正确}
$$

$$
z_C = (-1)\times 3 + 1 \times 3 -1 = -1 < 0 \tag{正确}
$$

这里可能会产生一个疑问：既然用 $z$ 值是否大于0这个条件就可以判断出分类是否正确，那么二分类理论中为什么还要用 $Logistic$ 函数做一次分类呢？

原因是这样的：只有 $z$ 值的话，我们只能知道是大于0还是小于0，并不能有效地进行反向传播，也就是说我们无法告诉神经网络反向传播的误差的力度有多大。比如 $z=5$ 和 $z=-1$ 相比，难度意味着前者的力度是后者的5倍吗？

而有了 $Logistic$ 分类计算后，得到的值是一个 $(0,1)$ 之间的概率，比如：当 $z=5$ 时，$Logistic(5) = 0.993$；当 $z=-1$ 时，$Logistic(-1)=0.269$。这两个数值的含义是这两个样本在分类区内的概率，前者概率为 $99.3%$，偏向正例，后者概率为 $26.9%$，偏向负例。然后再计算损失函数，就可以得到神经网络可以理解的反向传播误差，比如上面曾经计算过的 $loss_A,loss_B,loss_C$。

### 6.3.3 二分类的几何原理

几何方式：让所有正例样本处于直线的一侧，所有负例样本处于直线的另一侧，直线尽可能处于两类样本的中间。

#### 二分类函数的几何作用

二分类函数的最终结果是把正例都映射到图6-6中的上半部分的曲线上，而把负类都映射到下半部分的曲线上。

![](img/img11)

图6-6 $Logistic$ 函数把输入的点映射到 $(0,1)$ 区间内实现分类

我们用正例来举例：

$$a = Logistic(z) = \frac{1}{1 + e^{-z}} > 0.5$$

做公式变形，两边取自然对数，可以得到：

$$z > 0$$

即：
$$
z = x_1 \cdot w_1 + x_2 \cdot w_2 + b > 0
$$

对上式做一下变形，把$x_2$放在左侧，其他项放在右侧（假设$w_2>0$，则不等号方向不变）：
$$
x_2 > - \frac{w_1}{w_2}x_1 - \frac{b}{w_2} \tag{5}
$$

简化一下两个系数，令$w'=-w1/w2,b'=-b/w2$：

$$
x_2 > w' \cdot x_1 + b' \tag{6}
$$

公式6用几何方式解释，就是：有一条直线，方程为 $z = w' \cdot x_1+b'$，所有的正例样本都处于这条直线的上方；同理可得所有的负例样本都处于这条直线的下方。

![](img/img12.jpg)

图6-7 用直线分开的两类样本

## 学习过程

首先对ppt进行阅读梳理出一个大方向，选择一些重要的章节先进行学习，所谓先枝后叶，如果学习有些概念不懂，再对细节进行学习，ppt确定大方向后再对以及列好的在markdown中进行学习

## 学习心得体会

本章是沿着回归问题->逻辑回归问题->线性逻辑回归即分类问题->线性二分类问题展开的，我们最基本就是要先了解Sigmoid函数以及Logistic函数公式，然后结合step1学习的内容进行分析，重点还是线性二分类的原理。