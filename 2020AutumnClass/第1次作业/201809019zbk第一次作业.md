# 第一次作业

## 报告
### 对人工智能的理解
在学习Step1和慕课《商务数据分析》后，了解到深度学习的入门知识归纳成了9个步骤，简称为9步学习法：

1. 基本概念
2. 线性回归
3. 线性分类
4. 非线性回归
5. 非线性分类
6. 模型的推理与部署
7. 深度神经网络
8. 卷积神经网络
9. 循环神经网络

并且我们一般会使用如下方式进行：

1. 提出问题：先提出一个与现实相关的假想问题，为了由浅入深，这些问题并不复杂，是实际的工程问题的简化版本。
2. 解决方案：用神经网络的知识解决这些问题，从最简单的模型开始，一步步到复杂的模型。
3. 原理分析：使用基本的物理学概念或者数学工具，理解神经网络的工作方式。
4. 可视化理解：可视化是学习新知识的重要手段，由于我们使用了简单案例，因此可以很方便地可视化。

对于读者的要求：

1. 学过高等数学中的线性代数与微分
2. 有编程基础，可以不会Python语言，因为可以从示例代码中学得
3. 思考 + 动手的学习模式

可以帮助读者达到的水平：

1. 可以判断哪些任务是机器学习可以实现的，哪些是科学幻想，不说外行话
2. 深刻了解神经网络和深度学习的基本理论
3. 培养举一反三的解决实际问题的能力
4. 得到自学更复杂模型和更高级内容的能力
5. 对于天资好的读者，可以培养研发新模型的能力

## 符号约定

|符号|含义|
|---|---|
|$x$|训练用样本值|
|$x_1$|第一个样本或样本的第一个特征值，在上下文中会有说明|
|$x_{12},x_{1,2}$|第1个样本的第2个特征值|
|$X$|训练用多样本矩阵|
|$y$|训练用样本标签值|
|$y_1$|第一个样本的标签值|
|$Y$|训练用多样本标签矩阵|
|$z$|线性运算的结果值|
|$Z$|线性运算的结果矩阵|
|$Z1$|第一层网络的线性运算结果矩阵|
|$\sigma$|激活函数|
|$a$|激活函数结果值|
|$A$|激活函数结果矩阵|
|$A1$|第一层网络的激活函数结果矩阵|
|$w$|权重参数值|
|$w_{12},w_{1,2}$|权重参数矩阵中的第1行第2列的权重值|
|$w1_{12},w1_{1,2}$|第一层网络的权重参数矩阵中的第1行第2列的权重值|
|$W$|权重参数矩阵|
|$W1$|第一层网络的权重参数矩阵|
|$b$|偏移参数值|
|$b_1$|偏移参数矩阵中的第1个偏移值|
|$b2_1$|第二层网络的偏移参数矩阵中的第1个偏移值|
|$B$|偏移参数矩阵（向量）|
|$B1$|第一层网络的偏移参数矩阵（向量）|
|$X^T$|X的转置矩阵|
|$X^{-1}$|X的逆矩阵|
|$loss,loss(w,b)$|单样本误差函数|
|$J, J(w,b)$|多样本损失函数|

## 人工智能发展
- 简单的程序流程图

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/image2.png"  width="500" />

``` <img src=" ··· "  width="500" />   ···中间放图片链接```

-  人工智能发展史

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/image3.png" />

## 人工智能定义
人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。
人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。

机器学习可以大致分为三种类型：
1. 监督学习：
   
   通过标注的数据来学习，例如，程序通过学习标注了正确答案的手写数字的图像数据，它就能认识其他的手写数字。

2. 无监督学习：
   
   通过没有标注的数据来学习。这种算法可以发现数据中自然形成的共同特性（聚类），可以用来发现不同数据之间的联系，例如，买了商品A的顾客往往也购买了商品B。

3. 强化学习：
   
   我们可以让程序选择和它的环境互动（例如玩一个游戏），环境给程序的反馈是一些“奖励”（例如游戏中获得高分），程序要学习到一个模型，能在这种环境中得到高的分数，不仅是当前局面要得到高分，而且最终的结果也要是高分才行。

## M-P神经元模型

所谓M-P模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，它实际上就是对单个神经元的一种建模
<img src="https://upload-images.jianshu.io/upload_images/3403753-a50eadb886923c7a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1179/format/webp" width="500" />
1. 神经元输入分兴奋性输入和抑制性输入两种类型；

2. 神经元具有空间整合特性和阈值特性（兴奋和抑制，超过阈值为兴奋，低于是抑制）；

3. 神经元输入与输出间有固定的时滞，主要取决于突触延搁；

4. 每个神经元都是一个多输入单输出的信息处理单元；
   
   按照生物神经元，我们建立M-P模型
   <img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/image5.png" width="500" />

   训练AI模型，需要一系列专门的工具，业界有不少成熟的训练平台（TensorFlow，PyTorch，MXNet等），这些平台也在不断演化，支持新的模型，提高训练的效率，改进易用性，等等。
   2015年，AI取得了超过人类的成果。在其它的领域中，我们也看到了AI取得了达到或超过人类最高水平的成绩：

- 翻译领域（微软的中英翻译超过人类）
- 阅读理解（SQuAD 比赛）
- 下围棋（2016）德州扑克（2019）麻将（2019）

- 弱人工智能领域中机器学习部分的内容
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/image6.png" />

## 范式的演化

### 1.2.1 范式演化的四个阶段

#### 第一个阶段：经验
#### 第二个阶段：理论
由伽利略在比萨斜塔做实验，得出真空中自由落体下落的公式：
$$h = \frac{1}{2}gt^{2}$$  
 ```$$ $$  中间放公式 分式命令写法：\frac{分母}{分子}```

#### 第三个阶段：计算仿真
#### 第四个阶段：数据探索

### 范式各阶段的应用

- `智能之门问题`

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/image11.png" width="500" />

先`经验归纳` 由直觉告诉我们怎么选
---
再`理论推导` 用概率的基本方法来解题
---

设 $A$ 为第一次选到了中奖门的概率，$B$ 为改变选择后选到了中奖门的概率，$C$ 为未改变选择后选到了中奖门的概率。

$\displaystyle P(A)=\frac{1}{3}$ （初始选择就是获奖门的获奖概率是$\displaystyle \frac{1}{3}$）

$\displaystyle P(A')=\frac{2}{3}$ （当选中一个门之后， 其它两个门的获奖概率是$\displaystyle \frac{2}{3}$）

$P(B|A)=0$ （用户先选择了一个门，奖品在这个门后，用户后来改变选择，他的获奖概率是 $0$）

$P(C|A)=1$（用户选择了一个门，奖品在门后，后来他不改变选择，他的获奖概率是 $1$）

$P(B|A')=1$，$P(C|A')=0$（类似地， 用户首次选择的门后面没有奖品，他改变选择后，获奖概率是 $1$， 不改变选择，那么获奖概率是 $0$）

$\displaystyle P(B)=P(B|A) \times P(A) + P(B|A') \times P(A')=\frac{2}{3}$（所以，改变选择后中奖的概率，等于$\displaystyle \frac{2}{3}$）

$\displaystyle P(C)=P(C|A') \times P(A') + P(C|A) \times P(A)=\frac{1}{3}$（不改变选择而中奖的概率，等于$\displaystyle \frac{1}{3}$，和 $A$ 一样）

结论：$P(B)>P(C)$

---

最后可以用`数据模拟`的方法，来看各种情况
---
-  用`python`程序：
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/image12.png" width="600" />
当我们模拟一百万轮换门和不换门，得到了这样的结果：

- 换门：最后得奖的概率是 $0.666572$（约$\displaystyle \frac{2}{3}$）
- 不换门：最后得奖的概率是 $0.334115$（约$\displaystyle \frac{1}{3}$）
``` 
 字体控制是：\displaystyle
```
  
### 数据探索
伪代码的程序示例（使用Q-Learning的方法）：

```
//维护一个表格，命名为Q，存储Agent所在的“状态”和“收益”
1. 起始状态，表格Q所有内容都清零 
2. 在每一个选择的机会时候 
3.   查找目前状态中可能收益最大的动作
4.   执行动作，得到收益，进入下一个状态
5.   按照规则，更新表格Q 中的收益，规则是
6.     新收益 = 原来的收益 + （新收益 +未来可能获得的最大收益）* 折扣率
```

##  神经网络的基本工作原理简介

#### 求和计算 sum

$$
\begin{aligned}
Z &= w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 + b \\\\
&= \sum_{i=1}^m(w_i \cdot x_i) + b
\end{aligned}
$$

在上面的例子中 $m=3$。我们把$w_i \cdot x_i$变成矩阵运算的话，就变成了：

$$Z = W \cdot X + b$$

- 神经网络训练流程图
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/TrainFlow.png" />

#### 前提条件

 1. 首先是我们已经有了训练数据；
 2. 我们已经根据数据的规模、领域，建立了神经网络的基本结构，比如有几层，每一层有几个神经元；
 3. 定义好损失函数来合理地计算误差。


## 反向传播与梯度下降
反向传播与梯度下降的基本工作原理：

1. 初始化；
2. 正向计算；
3. 损失函数为我们提供了计算损失的方法；
4. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
5. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
6. Go to 2，直到精度足够好（比如损失函数值小于 $0.001$）。


##  线性反向传播

假设有一个函数：

$$z = x \cdot y \tag{1}$$

其中:

$$x = 2w + 3b \tag{2}$$

$$y = 2b + 1 \tag{3}$$
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/flow1.png"/>

当 $w = 3, b = 4$ 

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/flow2.png"/>

## 非线性反向传播

- 非线性的反向传播
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/game.png" ch="500" />
其中$1<x<=10,0<y<2.15$。假设有5个人分别代表 $x,a,b,c,y$：

正向与反向的迭代计算

|方向|公式|迭代1|迭代2|迭代3|迭代4|迭代5|
|---|---|---|---|---|---|---|
|正向|$x=x-\Delta x$|2|4.243|7.344|9.295|9.665|
|正向|$a=x^2$|4|18.005|53.934|86.404|93.233|
|正向|$b=\ln(a)$|1.386|2.891|3.988|4.459|4.535|
|正向|$c=\sqrt{b}$|1.177|1.700|1.997|2.112|2.129|
||标签值y|2.13|2.13|2.13|2.13|2.13|
|反向|$\Delta c = c - y$|-0.953|-0.430|-0.133|-0.018||
|反向|$\Delta b = \Delta c \cdot 2\sqrt{b}$|-2.243|-1.462|-0.531|-0.078||
|反向|$\Delta a = \Delta b \cdot a$|-8.973|-26.317|-28.662|-6.698||
|反向|$\Delta x = \Delta a / 2x$|-2.243|-3.101|-1.951|-0.360||

代码可以得到如下结果：

```
how to play: 1) input x, 2) calculate c, 3) input target number but not faraway from c
input x as initial number(1.2,10), you can try 1.3:
2
c=1.177410
input y as target number(0.5,2), you can try 1.8:
2.13
forward...
x=2.000000,a=4.000000,b=1.386294,c=1.177410
backward...
delta_c=-0.952590, delta_b=-2.243178, delta_a=-8.972712, delta_x=-2.243178
......
forward...
x=9.655706,a=93.232666,b=4.535098,c=2.129577
backward...
done!
```

第一步时`c=1.177410`，最后一步时`c=2.129577`，停止迭代。

##  损失函数概论

## 概念

在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本书中，使用“损失函数”和“Loss Function”这两个词汇，具体的损失函数符号用 $J$ 来表示，误差值用 $loss$ 表示。

“损失”就是所有样本的“误差”的总和，亦即（$m$ 为样本数）：

$$损失 = \sum^m_{i=1}误差_i$$

$$J = \sum_{i=1}^m loss_i$$

### 损失函数的作用

损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。
具体步骤：

1. 用随机值初始化前向计算公式的参数；
2. 代入样本，计算输出的预测值；
3. 用损失函数计算预测值和标签值（真实值）的误差；
4. 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
5. 进入第2步重复, 直到损失函数值达到一个满意的值就停止迭代。

###  机器学习常用损失函数

符号规则：$a$ 是预测值，$y$ 是样本标签值，$loss$ 是损失函数值。

- Gold Standard Loss，又称0-1误差
$$
loss=\begin{cases}
0 & a=y \\\\
1 & a \ne y 
\end{cases}
$$
``` \begin{cases} 是大括号 ```

#### 用二维函数图像理解单变量对损失函数的影响

- 单变量的损失函数图

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/gd2d.png" />

##  均方差函数

均方差函数常用于线性回归(linear regression)，即函数拟合(function fitting)。公式如下：

$$
loss = {1 \over 2}(z-y)^2 \tag{单样本}
$$

$$
J=\frac{1}{2m} \sum_{i=1}^m (z_i-y_i)^2 \tag{多样本}
$$

### 实际案例

假设有一组数据图，我们想找到一条拟合的直线。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/mse1.png" ch="500" />

前三张显示了一个逐渐找到最佳拟合直线的过程。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/mse2.png" ch="500" />
- 第一张，用均方差函数计算得到 $Loss=0.53$；
- 第二张，直线向上平移一些，误差计算 $Loss=0.16$，比图一的误差小很多；
- 第三张，又向上平移了一些，误差计算 $Loss=0.048$，此后还可以继续尝试平移（改变 $b$ 值）或者变换角度（改变 $w$ 值），得到更小的损失函数值；
- 第四张，偏离了最佳位置，误差值 $Loss=0.18$，这种情况，算法会让尝试方向反向向下。

我们假设该拟合直线的方程是 $y=2x+3$，当我们固定 $w=2$，把 $b$ 值从 $2$ 到 $4$ 变化时，损失函数值的变化
如图
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/LossWithB.png" ch="500" />

#### 损失函数值的2D示意图

损失函数的等高线图
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/lossfunction_contour.png" ch="500" />

用代码来表示：

```Python
    s = 200
    W = np.linspace(w-2,w+2,s)
    B = np.linspace(b-2,b+2,s)
    LOSS = np.zeros((s,s))
    for i in range(len(W)):
        for j in range(len(B)):
            z = W[i] * x + B[j]
            loss = CostFunction(x,y,z,m)
            LOSS[i,j] = round(loss, 2)
```
上述代码针对每个 $(w,b)$ 组合计算出了一个损失值，保留小数点后2位，放在`LOSS`矩阵中：
```
[[4.69 4.63 4.57 ... 0.72 0.74 0.76]
 [4.66 4.6  4.54 ... 0.73 0.75 0.77]
 [4.62 4.56 4.5  ... 0.73 0.75 0.77]
 ...
 [0.7  0.68 0.66 ... 4.57 4.63 4.69]
 [0.69 0.67 0.65 ... 4.6  4.66 4.72]
 [0.68 0.66 0.64 ... 4.63 4.69 4.75]]
```

当我们把所有值为0.72的点绘制成红色，把所有值为0.75的点绘制成蓝色
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/lossfunction2d.png" ch="500" />

##  交叉熵损失函数
交叉熵是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 $p,q$ 的差异，其中 $p$ 表示真实分布，$q$ 表示预测分布，那么 $H(p,q)$ 就称为交叉熵：

$$H(p,q)=\sum_i p_i \cdot \ln {1 \over q_i} = - \sum_i p_i \ln q_i \tag{1}$$


<img src="https://img-blog.csdn.net/20180914203608577?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlYXJ0aG91Z2Fu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" ch="500"/>

### 交叉熵的由来

信息论中，信息量的表示方式：

$$I(x_j) = -\ln (p(x_j)) \tag{2}$$

$x_j$：表示一个事件

$p(x_j)$：表示 $x_j$ 发生的概率

$I(x_j)$：信息量，$x_j$ 越不可能发生时，它一旦发生后的信息量就越大

#### 熵

$$H(p) = - \sum_j^n p(x_j) \ln (p(x_j)) \tag{3}$$

则上面的问题的熵是：

$$
\begin{aligned}  
H(p)&=-[p(x_1) \ln p(x_1) + p(x_2) \ln p(x_2) + p(x_3) \ln p(x_3)] \\\\
&=0.7 \times 0.36 + 0.2 \times 1.61 + 0.1 \times 2.30 \\\\
&=0.804
\end{aligned}
$$

###  二分类问题交叉熵
- 二分类交叉熵损失函数图
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/crossentropy2.png" ch="500" />

二分类对于批量样本的交叉熵计算公式是：

$$J= - \sum_{i=1}^m [y_i \ln a_i + (1-y_i) \ln (1-a_i)] \tag{1}$$
把公式1分解开两种情况，当 $y=1$ 时，即标签值是 $1$，是个正例，加号后面的项为 $0$：
$$loss = -\ln(a) \tag{2}$$



# Markdown数学公式语法

- 行内与独行:

1. 行内公式：将公式插入到本行内，符号：$公式内容$，如：$xyz$
2. 独行公式：将公式插入到新的一行内，并且居中，符号：$$公式内容$$，如：$$xyz$$

- 上标、下标与组合
  
1. 上标符号，符号：^，如：$x^4$
2. 下标符号，符号：_，如：$x_1$
3. 组合符号，符号：{}，如：${16}_{8}O{2+}_{2}$

- 汉字、字体与格式
1. 汉字形式，符号：\mbox{}，如：$V_{\mbox{初始}}$
2. 字体控制，符号：\displaystyle，如：$\displaystyle \frac{x+y}{y+z}$
3. 下划线符号，符号：\underline，如：$\underline{x+y}$
4. 标签，符号\tag{数字}，如：$\tag{11}$
5. 上大括号，符号：\overbrace{算式}，如：$\overbrace{a+b+c+d}^{2.0}$
6. 下大括号，符号：\underbrace{算式}，如：$a+\underbrace{b+c}_{1.0}+d$
7. 上位符号，符号：\stacrel{上位符号}{基位符号}，如：$\vec{x}\stackrel{\mathrm{def}}{=}{x_1,\dots,x_n}$

- 占位符
1. 两个quad空格，符号：\qquad，如：$x \qquad y$
2. quad空格，符号：\quad，如：$x \quad y$
3. 大空格，符号\，如：$x \ y$
4. 中空格，符号\:，如：$x : y$
5. 小空格，符号\,，如：$x , y$
6. 没有空格，符号``，如：$xy$
7. 紧贴，符号\!，如：$x ! y$

- 定界符与组合
1. 括号，符号：（）\big(\big) \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)，如：$（）\big(\big) \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)$
2. 中括号，符号：[]，如：$[x+y]$
3. 大括号，符号：\{ \}，如：${x+y}$
4. 自适应括号，符号：\left \right，如：$\left(x\right)$，$\left(x{yz}\right)$
5. 组合公式，符号：{上位公式 \choose 下位公式}，如：${n+1 \choose k}={n \choose k}+{n \choose k-1}$
6. 组合公式，符号：{上位公式 \atop 下位公式}，如：$\sum_{k_0,k_1,\ldots>0 \atop k_0+k_1+\cdots=n}A_{k_0}A_{k_1}\cdots$

- 四则运算
1. 加法运算，符号：+，如：$x+y=z$
2. 减法运算，符号：-，如：$x-y=z$
3. 加减运算，符号：\pm，如：$x \pm y=z$
4. 减甲运算，符号：\mp，如：$x \mp y=z$
5. 乘法运算，符号：\times，如：$x \times y=z$
6. 点乘运算，符号：\cdot，如：$x \cdot y=z$
7. 星乘运算，符号：\ast，如：$x \ast y=z$
8. 除法运算，符号：\div，如：$x \div y=z$
9. 斜法运算，符号：/，如：$x/y=z$
10. 分式表示，符号：\frac{分子}{分母}，如：$\frac{x+y}{y+z}$
11. 分式表示，符号：{分子} \voer {分母}，如：${x+y} \over {y+z}$
12. 绝对值表示，符号：||，如：$|x+y|$

- 高级运算
1. 平均数运算，符号：\overline{算式}，如：$\overline{xyz}$
2. 开二次方运算，符号：\sqrt，如：$\sqrt x$
3. 开方运算，符号：\sqrt[开方数]{被开方数}，如：$\sqrt[3]{x+y}$
4. 对数运算，符号：\log，如：$\log(x)$
5. 极限运算，符号：\lim，如：$\lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}$
6. 极限运算，符号：\displaystyle \lim，如：$\displaystyle \lim^{x \to \infty}_{y \to 0}{\frac{x}{y}}$
7. 求和运算，符号：\sum，如：$\sum^{x \to \infty}_{y \to 0}{\frac{x}{y}}$
8. 求和运算，符号：\displaystyle \sum，如：$\displaystyle \sum^{x \to \infty}_{y \to 0}{\frac{x}{y}}$
9. 积分运算，符号：\int，如：$\int^{\infty}_{0}{xdx}$
10. 积分运算，符号：\displaystyle \int，如：$\displaystyle \int^{\infty}_{0}{xdx}$
11. 微分运算，符号：\partial，如：$\frac{\partial x}{\partial y}$
12. 矩阵表示，符号：\begin{matrix} \end{matrix}，如：$\left[ \begin{matrix} 1 &2 &\cdots &4\5 &6 &\cdots &8\\vdots &\vdots &\ddots &\vdots\13 &14 &\cdots &16\end{matrix} \right]$