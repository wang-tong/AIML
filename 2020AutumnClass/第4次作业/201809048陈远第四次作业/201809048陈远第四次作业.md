## 深度神经网络搭建

## 1，神经网络的基本构架

### 1.1 神经网络

神经网络可以看作为一个复合函数，当你输入一些数据时，它便对应输出相应的数据。首先我们来看神经元的结构
1.1.1，神经元
将一个神经元看做一个函数，里面包含了函数所需要的数据和操作，以函数y=ax+b来说这个神经元，数据首先输入到这个神经元然后将权重a与数据x相乘，然后再向相乘的数据添加偏置项b,最后从另一端输出.然后再通过激活函数将数据x的输出限制在一个范围内.总的来说就是从一个数据点传入数据，计算多个输出值，再传递到下一个单元，这样一系列的操作便组成了一个神经网络。

1.1.2，神经网络中的数据
连接是神经网络必须学习的数值.
权重代表神经网络认为权重与数据相乘之后应该添加的内容.
超参数是改变机器行为的一系列数据就是超参数，超参数需要手动设置.
偏置项：有利于神经网络的收敛和数据的精确度.

### 1.2 深度神经网络的模块划分

批量输入模块
各种深度学习零件搭建的深度学习网络
优化模块

### 1.3 深度神经网络常用层

1.3.1，dense层，activation层，dropout层， flatten层

1.3.2 卷积层
卷积就是一种把相对应位置数据相乘然后再相加的运算，卷积神经网络可以用更少的参数对数据进行计算，得到最终的数据处理结果，卷积神经网络是由输入层，卷积层，激活函数，池化层，全连接层组成。

1.3.3 池化层
池化层可以非常有效的缩小参数矩阵的尺寸，从而减少最后dense层中的参数数量，加快计算速度，防止过拟合。

1.3.4 正则化层
正则化就是规则化，为给需要训练的目标函数加上一些限制，让它们不要超过一定的范围，也是用来防止过拟合。

1.3.5 反卷积层
卷积层通过矩阵来理解 正向和反向分别乘以c和ct。反卷积就是正向和反向分别乘以ct和c反卷积就是将输入范围变大在实际情况中，输出的范围是大于输入的范围的要想还原输入，实际上是无法还原的，但是通过反卷积可以最大程度上进行模拟学习的得到。

## 2，回归任务功能测试

### 2.1，搭建模型

模型如下所示：一个双层的神经网络，第一层后面接一个Sigmoid激活函数，第二层直接输出拟合数据

![ch1](ch1.png)

代码如下：

```Python
def model():
    dataReader = LoadData()(进行数据的读取)
    num_input = 1
    num_hidden1 = 4
    num_output = 1

    max_epoch = 10000
    batch_size = 10
    learning_rate = 0.5

    params = HyperParameters_4_0(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.Fitting,（调用fittingh函数）
        init_method=InitialMethod.Xavier,(调用xavier函数)
        stopper=Stopper(StopCondition.StopLoss, 0.001))

    net = NeuralNet_4_0(params, "Level1_CurveFittingNet")
    (调用param参数)
    fc1 = FcLayer_1_0(num_input, num_hidden1, params)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    sigmoid1 = ActivationLayer(Sigmoid())(调用激活函数)
    net.add_layer(sigmoid1, "sigmoid1")(调用sigmoid参数)
    fc2 = FcLayer_1_0(num_hidden1, num_output, params)
    net.add_layer(fc2, "fc2")(调用fc2参数)

    net.train(dataReader, checkpoint=100, need_test=True)(神经网络样本训练)

    net.ShowLossHistory()
    ShowResult(net, dataReader)
```

超参数说明：

1. 输入层1个神经元，因为只有一个x值
2. 隐层4个神经元，对于此问题来说应该是足够了，因为特征很少
3. 输出层1个神经元，因为是拟合任务
4. 学习率=0.5
5. 最大epoch=10000轮
6. 批量样本数=10
7. 拟合网络类型
8. Xavier初始化
9. 绝对损失停止条件=0.001

### 2.2，训练结果

![ch2](ch2.png)
由上面的图可以看出：损失函数值在一段平缓期过后，开始陡降，这种现象在神经网络的训练中是常见的，最有可能的是当时处于一个梯度变化的平缓地带，算法在艰难地寻找下坡路，然后忽然就找到了。

![ch3](ch3.png)
上图左侧子图是拟合的情况，绿色点是测试集数据，红色点是神经网路的推理结果，可以看到除了最左侧开始的部分，其它部分都拟合相对较好。

右侧的子图是用下面的代码生成的：

```Python
    y_test_real = net.inference(dr.XTest)
    axes.scatter(y_test_real, y_test_real-dr.YTestRaw, marker='o')
```

以测试集的真实值为横坐标，以真实值和预测值的差为纵坐标。最理想的情况是所有点都在y=0处排成一条横线。从图上看，真实值和预测值二者的差异明显，但是请注意横坐标和纵坐标的间距相差一个数量级，所以差距其实不大。

再看打印输出的最后部分：

```
epoch=4999, total_iteration=449999
loss_train=0.000920, accuracy_train=0.968329
loss_valid=0.000839, accuracy_valid=0.962375
time used: 28.002626419067383
save parameters
total weights abs sum= 45.27530164993504
total weights = 8
little weights = 0
zero weights = 0
testing...
0.9817814550687021
0.9817814550687021
```

由于我们设置了eps=0.001，所以在5000多个epoch时便达到了要求，训练停止。最后用测试集得到的准确率为98.17%，已经非常不错了。如果训练更多的轮，可以得到更好的结果。

## 3 二分类任务功能测试

### 3.1 搭建模型

二分类任务模型同样是一个双层神经网络，但是最后一层要接一个Logistic二分类函数来完成二分类任务

![ch4](ch4.png)

代码如下：

```Python

def model(dataReader):
    num_input = 2
    num_hidden = 3
    num_output = 1

    max_epoch = 1000
    batch_size = 5
    learning_rate = 0.1

    params = HyperParameters_4_0(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.BinaryClassifier,(调用binaryclassifier函数)
        init_method=InitialMethod.Xavier,(调用xiavier函数)
        stopper=Stopper(StopCondition.StopLoss, 0.02))

    net = NeuralNet_4_0(params, "Arc")

    fc1 = FcLayer_1_0(num_input, num_hidden, params)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    sigmoid1 = ActivationLayer(Sigmoid())(调用sigmoid1激活函数)
    net.add_layer(sigmoid1, "sigmoid1")(调用sigmoid1参数)
    fc2 = FcLayer_1_0(num_hidden, num_output, params)
    net.add_layer(fc2, "fc2")(调用fc2参数)
    logistic = ClassificationLayer(Logistic())(调用logistic函数)
    net.add_layer(logistic, "logistic")(调用logistic参数)

    net.train(dataReader, checkpoint=10, need_test=True)(神经网络样本训练)
    return net
```

超参数说明：

1. 输入层神经元数为2
2. 隐层的神经元数为3，使用Sigmoid激活函数
3. 由于是二分类任务，所以输出层只有一个神经元，用Logistic做二分类函数
4. 最多训练1000轮
5. 批大小=5
6. 学习率=0.1
7. 绝对误差停止条件=0.02

### 3.2 运行结果

![ch10](ch10.png)
改变相关参数的运行结果图如下图所示：
![ch7](ch7.png)

```
epoch=419, total_iteration=30239
loss_train=0.010094, accuracy_train=1.000000
loss_valid=0.019141, accuracy_valid=1.000000
time used: 2.149379253387451
testing...
1.0
```

最后的testing...的结果是1.0，表示100%正确，这初步说明mini框架在这个基本case上工作得很好。下图分类结果也做的不错。

![ch5](ch5.png)

## 4. 多分类功能测试

在第11章里，我们讲解了如何使用神经网络做多分类。在本节我们将会用Mini框架重现那个教学案例，然后使用一个真实的案例验证多分类的用法。

### 4.1 搭建模型一

使用Sigmoid做为激活函数的两层网络
![ch9](ch9.png)

代码如下：

```Python
def model_sigmoid(num_input, num_hidden, num_output, hp):
    net = NeuralNet_4_0(hp, "chinabank_sigmoid")(调用hp参数)

    fc1 = FcLayer_1_0(num_input, num_hidden, hp)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    s1 = ActivationLayer(Sigmoid())
    net.add_layer(s1, "Sigmoid1")(调用s1参数)

    fc2 = FcLayer_1_0(num_hidden, num_output, hp)
    net.add_layer(fc2, "fc2")(调用fc2参数)
    softmax1 = ClassificationLayer(Softmax())
    net.add_layer(softmax1, "softmax1")(调用softmax1参数)

    net.train(dataReader, checkpoint=50, need_test=True)(神经网络样本训练)
    net.ShowLossHistory()
    ShowResult(net, hp.toString())
    ShowData(dataReader)
```

超参数说明

1. 隐层8个神经元
2. 最大`epoch=5000`
3. 批大小=10
4. 学习率0.1
5. 绝对误差停止条件=0.08
6. 多分类网络类型
7. 初始化方法为Xavier

`net.train()`函数是一个阻塞函数，只有当训练完毕后才返回。

#### 4.2运行结果

训练过程和分类效果如下图所示：
![ch7](ch7.png)
![ch11](ch11.png)

### 5.1 搭建模型二

使用ReLU做为激活函数的三层网络
![ch8](ch8.png)

用两层网络也可以实现，但是使用ReLE函数时，训练效果不是很稳定，用三层比较保险。

代码如下

```Python
def model_relu(num_input, num_hidden, num_output, hp):
    net = NeuralNet_4_0(hp, "chinabank_relu")(调用hp参数)

    fc1 = FcLayer_1_0(num_input, num_hidden, hp)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    r1 = ActivationLayer(Relu())
    net.add_layer(r1, "Relu1")(调用r1参数)

    fc2 = FcLayer_1_0(num_hidden, num_hidden, hp)
    net.add_layer(fc2, "fc2")(调用fc2参数)
    r2 = ActivationLayer(Relu())
    net.add_layer(r2, "Relu2")(调用r2参数)

    fc3 = FcLayer_1_0(num_hidden, num_output, hp)
    net.add_layer(fc3, "fc3")(调用fc3参数)
    softmax = ClassificationLayer(Softmax())(调用softmax函数)
    net.add_layer(softmax, "softmax")(调用softmax参数)

    net.train(dataReader, checkpoint=50, need_test=True)(神经网络样本训练)
    net.ShowLossHistory()
    ShowResult(net, hp.toString())
    ShowData(dataReader)
```

超参数说明

1. 隐层8个神经元
2. 最大`epoch=5000`
3. 批大小=10
4. 学习率0.1
5. 绝对误差停止条件=0.08
6. 多分类网络类型
7. 初始化方法为MSRA

#### 5.2运行结果

训练过程和分类效果如图下图所示
![ch6](ch6.png)
![ch12](ch12.png)

### 5.3 比较

下图是使用不同的激活函数的分类效果图和使用不同的激活函数的分类结果的比较。

|Sigmoid|ReLU|
|---|---|
|![ch11](ch11.png)|![ch12](ch12.png)|

从图片中可以看到左图中的边界要平滑许多，这也就是ReLU和Sigmoid的区别。ReLU是用分段线性拟合曲线，Sigmoid有真正的曲线拟合能力。但是Sigmoid也有缺点，看分类的边界，使用ReLU函数的分类边界比较清晰，而使用Sigmoid函数的分类边界要平缓一些，过渡区较宽。
Relu能直则直，对方形边界适用；Sigmoid能弯则弯，对圆形边界适用。

## 6 权重矩阵初始化

### 6.1 零初始化

首先我们就要谈到神经网络的权重初始化方法对模型的收敛速度和性能有着至关重要的影响。神经网络其实就是对权重参数w的不停迭代更新，以期达到较好的性能。在深度神经网络中，随着层数的增多，我们在梯度下降的过程中，极易出现梯度消失或者梯度爆炸。因此，对权重的初始化则显得至关重要，一个好的权重初始化虽然不能完全解决梯度消失和梯度爆炸的问题，但是对于处理这两个问题是有很大的帮助的，并且十分有利于模型性能和收敛速度。

零初始化即把神经网络所有层的权重`W`值的初始值都设置为0。

$$
W = 0
$$

但是对于多层网络来说，绝对不能用零初始化，否则权重值不能学习到合理的结果。看下面的零值初始化的权重矩阵值打印输出：
```
W1= [[-0.82452497 -0.82452497 -0.82452497]]
B1= [[-0.01143752 -0.01143752 -0.01143752]]
W2= [[-0.68583865]
 [-0.68583865]
 [-0.68583865]]
B2= [[0.68359678]]
```

可以看到`W1`、`B1`、`W2`内部3个单元的值都一样，这是因为初始值都是0，所以梯度均匀回传，导致所有`W`的值都同步更新，没有差别。这样的话，无论多少轮，最终的结果也不会正确。
下面还有几种初始化方法在此就不过多的叙述就看一下运行图片和代码吧！

### 6.2 标准初始化
基于激活函数sigmoid
![ch13](ch13.png)

### 6.3 Xavier初始化

基于激活函数sigmoid
![ch14](ch14.png)

基于激活函数relu
![ch16](ch16.png)

### 6.4 MSRA初始化

基于激活函数relu
![ch15](ch15.png)

代码如下：

```Python
def net(init_method, activator):

    max_epoch = 1
    batch_size = 5
    learning_rate = 0.02

    params = HyperParameters_4_1(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.Fitting,
        init_method=init_method)

    net = NeuralNet_4_1(params, "level1")
    num_hidden = [128,128,128,128,128,128,128](样本矩阵)
    fc_count = len(num_hidden)-1(样本矩阵的长度)
    layers = []

    for i in range(fc_count):
        fc = FcLayer_1_1(num_hidden[i], num_hidden[i+1], params)
        net.add_layer(fc, "fc")(调用fc参数)
        layers.append(fc)

        ac = ActivationLayer(activator)
        net.add_layer(ac, "activator")
        layers.append(ac)
    # end for
    # 从正态分布中取1000个样本，每个样本有num_hidden[0]个特征值
    # 转置是为了可以和w1做矩阵乘法
    x = np.random.randn(1000, num_hidden[0])

    # 激活函数输出值矩阵列表
    a_value = []

    # 依次做所有层的前向计算
    input = x
    for i in range(len(layers)):
        output = layers[i].forward(input)
        # 但是只记录激活层的输出
        if isinstance(layers[i], ActivationLayer):
            a_value.append(output)
        # end if
        input = output
    # end for

    for i in range(len(a_value)):
        ax = plt.subplot(1, fc_count+1, i+1)
        ax.set_title("layer" + str(i+1))
        plt.ylim(0,10000)
        if i > 0:
            plt.yticks([])
        ax.hist(a_value[i].flatten(), bins=25, range=[0,1])
    #end for
    # super title
    plt.suptitle(init_method.name + " : " + activator.get_name())
    plt.show()

if __name__ == '__main__':
    net(InitialMethod.Normal, Sigmoid())
    net(InitialMethod.Xavier, Sigmoid())
    net(InitialMethod.Xavier, Relu())
    net(InitialMethod.MSRA, Relu())
```

## 7 梯度下降优化算法

通过不停的迭代从而得到下面的图像
![ch17](ch17.png)
![ch23](ch23.png)
![ch24](ch24.png)
![ch25](ch25.png)
![ch26](ch26.png)

## 8 自适应学习率算法

通过不停的迭代从而得到下面的图像
![ch20](ch20.png)
![ch18](ch18.png)
![ch19](ch19.png)
![ch21](ch21.png)
![ch22](ch22.png)

## 9 正则化

### 9.1 拟合程度比较

正则化是用于防止过拟合。在深度神经网络中，我们经常遇到问题就是网络的泛化。所谓泛化，就是模型在测试集上的表现要和训练集上一样好。经常有这样的例子：一个模型在训练集上千锤百炼，能到达99%的准确率，拿到测试集上一试，准确率还不到90%。这说明模型过度拟合了训练数据，而不能反映真实世界的情况。解决过度拟合的手段和过程，就叫做泛化。
神经网络的两大功能：回归和分类。这两类任务，都会出现欠拟合和过拟合现象。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/16/fitting.png" />
上图是回归任务中的三种情况，依次为：欠拟合、正确的拟合、过拟合。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/16/classification.png" />
上图是分类任务中的三种情况，依次为：分类欠妥、正确的分类、分类过度。由于分类可以看作是对分类边界的拟合，所以我们经常也统称其为拟合。

上图中对于位置很特别的那颗绿色点样本，正确的做法是把它当作噪音看待，而不要让它对网络产生影响。而对于上例中的欠拟合情况，如果简单的（线性）模型不能很好地完成任务，我们可以考虑使用复杂的（非线性或深度）模型，即加深网络的宽度和深度，提高神经网络的能力。

但是如果网络过于宽和深，就会出现第三张图展示的过拟合的情况。

出现过拟合的原因：

1. 训练集的数量和模型的复杂度不匹配，样本数量级小于模型的参数
2. 训练集和测试集的特征分布不一致
3. 样本噪音大，使得神经网络学习到了噪音，正常样本的行为被抑制
4. 迭代次数过多，过分拟合了训练数据，包括噪音部分和一些非重要特征

用复杂度不匹配的模型的两个原因：

1. 因为有的模型以及非常成熟了，比如VGG16，可以不调参而直接用于你自己的数据训练，此时如果你的数据数量不够多，但是又想使用现有模型，就需要给模型加正则项了。
2. 使用相对复杂的模型，可以比较快速地使得网络训练收敛，以节省时间。

正则化后的数据图像如下图所示：
![ch29](ch29.png)
![ch27](ch27.png)
![ch28](ch28.png)
代码如下：

```Python
def Model(dataReader, num_input, num_hidden, num_output, params):
    net = NeuralNet_4_2(params, "overfitting")
    fc1 = FcLayer_2_0(num_input, num_hidden, params)
    net.add_layer(fc1, "fc1")(调用fc1参数)
    s1 = ActivatorLayer(Sigmoid())
    net.add_layer(s1, "s1")
    fc2 = FcLayer_2_0(num_hidden, num_hidden, params)
    net.add_layer(fc2, "fc2")(调用fc2参数)
    t2 = ActivatorLayer(Tanh())
    net.add_layer(t2, "t2")(调用t2参数)
    fc3 = FcLayer_2_0(num_hidden, num_hidden, params)
    net.add_layer(fc3, "fc3")(调用fc3参数)
    t3 = ActivatorLayer(Tanh())
    net.add_layer(t3, "t3")(调用t3参数)
    fc4 = FcLayer_2_0(num_hidden, num_output, params)
    net.add_layer(fc4, "fc4")(调用fc4参数)
    net.train(dataReader, checkpoint=100, need_test=True)
    net.ShowLossHistory(XCoordinate.Epoch)
    return net
```

## 10 L1正则合L2正则

### 10.1 L2正则化

L2 正则化公式非常简单，直接在原来的损失函数基础上加上权重参数的平方和：
$$L = E+\lambda\sum_j\lvert w_j^2 \$$
其中，E 是未包含正则化项的训练样本误差，λ 是正则化参数。

假设：

- W参数服从高斯分布，即：$w_j \sim N(0,\tau^2)$
- Y服从高斯分布，即：$y_i \sim N(w^Tx_i,\sigma^2)$

贝叶斯最大后验估计：

$$
\arg\max_wL(w) = \ln \prod_i^n \frac{1}{\sigma\sqrt{2 \pi}}\exp(-(\frac{y_i-w^Tx_i}{\sigma})^2/2) \cdot \prod_j^m{\frac{1}{\tau\sqrt{2\pi}}\exp(-(\frac{w_j}{\tau})^2/2)}
$$

$$
=-\frac{1}{2\sigma^2}\sum_i^n(y_i-w^Tx_i)^2-\frac{1}{2\tau^2}\sum_j^m{w_j^2}-n\ln\sigma\sqrt{2\pi}-m\ln \tau\sqrt{2\pi} \tag{3}
$$

因为$\sigma,b,n,\pi,m$等都是常数，所以损失函数$J(w)$的最小值可以简化为：

$$
\arg\min_wJ(w) = \sum_i^n(y_i-w^Tx_i)^2+\lambda\sum_j^m{w_j^2} \tag{4}
$$

看公式4，相当于是线性回归的均方差损失函数，再加上一个正则项，共同构成损失函数。如果想求这个函数的最小值，则需要两者协调，并不是说分别求其最小值就能实现整体最小，因为它们具有共同的W项，当W比较大时，第一项比较小，第二项比较大，或者正好相反。所以它们是矛盾组合体。

L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项最小，可以使得W的每个元素都很小，都接近于0，因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是“抗扰动能力强”。

代码如下：

```Python
if __name__ == '__main__':
    dr = LoadData()
    hp, num_hidden = SetParameters()
    hp.regular_name = RegularMethod.L2
    hp.regular_value = 0.01
    net = Model(dr, 1, num_hidden, 1, hp)
    ShowResult(net, dr, hp.toString())

```

运行结果如下图所示：
![ch32](ch32.png)
![ch30](ch30.png)
![ch31](ch31.png)

### 10.2 L1正则化

L1正则化是在机器学习的Loss函数中，通常会添加一些正则化（正则化与一些贝叶斯先验本质上是一致的，L2正则化与高斯先验是一致的、L1正则化与拉普拉斯先验是一致）来降低模型的结构风险，这样可以使降低模型复杂度、防止参数过大
假设：

- W参数服从拉普拉斯分布，即$w_j \sim Laplace(0,b)$
- Y服从高斯分布，即$y_i \sim N(w^Tx_i,\sigma^2)$

贝叶斯最大后验估计：
$$
\begin{aligned}
\arg\max_wL(w) = &\ln \prod_i^n \frac{1}{\sigma\sqrt{2 \pi}}\exp(-\frac{1}{2}(\frac{y_i-w^Tx_i}{\sigma})^2) 
\cdot \prod_j^m{\frac{1}{2b}\exp(-\frac{\lvert w_j \rvert}{b})}
\\\\
=&-\frac{1}{2\sigma^2}\sum_i^n(y_i-w^Tx_i)^2-\frac{1}{2b}\sum_j^m{\lvert w_j \rvert}
-n\ln\sigma\sqrt{2\pi}-m\ln b\sqrt{2\pi} 
\end{aligned}
\tag{1}
$$

因为$\sigma,b,n,\pi,m$等都是常数，所以损失函数$J(w)$的最小值可以简化为：

$$
\arg\min_wJ(w) = \sum_i^n(y_i-w^Tx_i)^2+\lambda\sum_j^m{\lvert w_j \rvert} \tag{2}
$$

我们仍以两个参数为例，公式2的后半部分的正则形式为：

$$L_1 = \lvert w_1 \rvert + \lvert w_2 \rvert \tag{3}$$

因为$w_1,w_2$有可能是正数或者负数，我们令$x=|w_1|,y=|w_2|,c=L_1$，则公式3可以拆成以下4个公式的组合：

$$
y=-x+c \quad (当w_1 \gt 0, w_2 \gt 0时)
$$
$$
y=\quad x+c \quad (当w_1 \lt 0, w_2 \gt 0时)
$$
$$
y=\quad x-c \quad (当w_1 \gt 0, w_2 \lt 0时)
$$
$$
y=-x-c \quad (当w_1 \lt 0, w_2 \lt 0时)
$$

代码如下：

```Python
if __name__ == '__main__':
    dr = LoadData()
    hp, num_hidden = SetParameters()
    hp.regular_name = RegularMethod.L1
    hp.regular_value = 0.005
    net = Model(dr, 1, num_hidden, 1, hp)
    ShowResult(net, dr, hp.toString())

```

运行结果如下图所示:
![ch33](ch33.png)
![ch34](ch34.png)
![ch35](ch35.png)

## 总结：

在构建深度神经网络时，ReLU 是最好的非线性（激活函数），但说实话，ReLU 确实是运行速度最快、最简便的，而且令人惊讶的是，它们在工作时梯度并不会逐渐减小（从而能够防止梯度消失）。尽管 sigmoid 是一个常用激活函数，但是它在 DNN 中传播梯度的效果并不太好。
不要在输出层使用激活函数。这应该是显而易见的，但是如果你通过一个共用的函数构建每一层，那这可能是一个很容易犯的错误：请确保在输出层不要使用激活函数。
为每一层添加一个偏置项。这是机器学习的入门知识：本质上，偏置项将一个平面转换到最佳拟合位置。在 y=mx+b 式中，b 是偏置项，使直线能够向上或向下移动到最佳的拟合位置。

在深度学习中，对神经网络的权重进行初始化对模型的收敛速度和性能的提升有着重要的影响。而且在神经网络在计算过程中需要对权重参数w不断的迭代更新，已达到较好的性能效果。但在训练的过程中，会遇到梯度消失和梯度爆炸等现象。因此，一个好的初始化权重能够对这两个问题有很好的帮助，并且，初始化权重能够有利于模型性能的提升，以及增快收敛速度。

梯度下降优化算法
1,BGD
优点：
对于凸目标函数，可以保证全局最优； 对于非凸目标函数，可以保证一个局部最优。
缺点：
速度慢; 数据量大时不可行; 无法处理动态产生的新样本。
2,SGD
优点：
更新频次快，优化速度更快; 可以无法处理动态产生的新样本；一定的随机性导致有几率跳出局部最优(随机性来自于用一个样本的梯度去代替整体样本的梯度)
缺点：
随机性可能导致收敛复杂化，即使到达最优点仍然会进行过度优化，因此SGD得优化过程相比BGD充满动荡；

正则化的目的：防止过拟合
正则化的本质：约束要优化的参数
L1正则化在实际中往往替代L0正则化，来防止过拟合。
L1正则化之所以可以防止过拟合，是因为L1范数就是各个参数的绝对值相加得到的，我们前面讨论了，参数值大小和模型复杂度是成正比的。因此复杂的模型，其L1范数就大，最终导致损失函数就大，说明这个模型就不够好。
L2正则化可以防止过拟合的原因和L1正则化一样，只是形式不太一样。
L2范数是各参数的平方和再求平方根，我们让L2范数的正则项最小，可以使W的每个元素都很小，都接近于0。但与L1范数不一样的是，它不会是每个元素为0，而只是接近于0。越小的参数说明模型越简单，越简单的模型越不容易产生过拟合现象。


