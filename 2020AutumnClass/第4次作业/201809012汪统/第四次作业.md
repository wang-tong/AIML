搭建交互式应用的实现步骤有：

1. 重现神经网络结构，加载权重和偏移数据
2. 显示界面，让用户可以用鼠标或者手指（触摸屏）写一个数字
3. 写好一个数字后，收集数据，转换数据，并触发推理过程
4. 得到推理结果
5. 清除界面，做下一次测试

`Python
# 加载权重和偏移数据
net = LoadNet()
# 注册事件
fig, ax = plt.subplots()
# 键盘事件
fig.canvas.mpl_connect('key_press_event', on_key_press)
# 鼠标释放
fig.canvas.mpl_connect('button_release_event', on_mouse_release)
# 鼠标按下
fig.canvas.mpl_connect('button_press_event', on_mouse_press)
# 鼠标移动
fig.canvas.mpl_connect('motion_notify_event', on_mouse_move)
# 设置固定的绘图尺寸
plt.axis([0,1,0,1])
plt.show()
```

事件响应逻辑：

- 在鼠标按下事件中，启动绘图功能
- 在鼠标移动事件中，检查如果绘图功能开启，就在面板上显示鼠标轨迹
- 在鼠标释放事件中，关闭绘图功能
- 在键盘事件中，如果收到回车键，就触发推理过程；如果收到回退键，就清空画板
数据处理过程如下：

1. 应用程序会先把绘图区域保存为一个文件
2. 然后再把此文件读入内存，转换成灰度图
3. 缩放尺寸到$28\times 28$（和训练数据一致）
4. 用255减去所有像素值，得到黑底色白前景色的数据（和训练数据一致）
5. 归一化到[0,1]（和训练数据一致）
6. 变成$1\times 784$的数组，调用前向计算方法
7. 得到`Output`后，做一个`argmax`，取到最终结果
完成房价预测任务的抽象模型

```Python
def model():
    dr = LoadData()

    num_input = dr.num_feature
    num_hidden1 = 32
    num_hidden2 = 16
    num_hidden3 = 8
    num_hidden4 = 4
    num_output = 1

    max_epoch = 1000
    batch_size = 16
    learning_rate = 0.1

    params = HyperParameters_4_0(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.Fitting,
        init_method=InitialMethod.Xavier,
        stopper=Stopper(StopCondition.StopDiff, 1e-7))

    net = NeuralNet_4_0(params, "HouseSingle")

    fc1 = FcLayer_1_0(num_input, num_hidden1, params)
    net.add_layer(fc1, "fc1")
    r1 = ActivationLayer(Relu())
    net.add_layer(r1, "r1")
    ......
    fc5 = FcLayer_1_0(num_hidden4, num_output, params)
    net.add_layer(fc5, "fc5")

    net.train(dr, checkpoint=10, need_test=True)
    
    output = net.inference(dr.XTest)
    real_output = dr.DeNormalizeY(output)
    mse = np.sum((dr.YTestRaw - real_output)**2)/dr.YTest.shape[0]/10000
    print("mse=", mse)
    
    net.ShowLossHistory()

    ShowResult(net, dr)
```

深度神经网络指的是微软推出了一新款语音识别软件，其工作原理是模仿人脑思考方式，从而使该软件的语音识别速度更快，识别准确率也更高。

多层的好处是可以用较少的参数表示复杂的函数。

在监督学习中，以前的多层神经网络的问题是容易陷入局部极值点。如果训练样本足够充分覆盖未来的样本，那么学到的多层权重可以很好的用来预测新的测试样本。但是很多任务难以得到足够多的标记样本，在这种情况下，简单的模型，比如线性回归或者决策树往往能得到比多层神经网络更好的结果（更好的泛化性，更差的训练误差）。

非监督学习中，以往没有有效的方法构造多层网络。多层神经网络的顶层是底层特征的高级表示，比如底层是像素点，上一层的结点可能表示横线，三角； 而顶层可能有一个结点表示人脸。一个成功的算法应该能让生成的顶层特征最大化的代表底层的样例。如果对所有层同时训练，时间复杂度会太高； 如果每次训练一层，偏差就会逐层传递。这会面临跟上面监督学习中相反的问题，会严重欠拟合。

2006年，hinton提出了在非监督数据上建立多层神经网络的一个有效方法，简单的说，分为两步，一是每次训练一层网络，二是调优使原始表示x向上生成的高级表示r和该高级表示r向下生成的x'尽可能一致。方法是

1，首先逐层构建单层神经元，这样每次都是训练一个单层网络。

2，当所有层训练完后，hinton使用wake-sleep算法进行调优。将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于”认知“，向下的权重用于”生成“。然后使用Wake-Sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。比如顶层的一个结点表示人脸，那么所有人脸的图像应该激活这个结点，并且这个结果向下生成的图像应该能够表现为一个大概的人脸图像。Wake-Sleep算法分为醒(wake)和睡(sleep)两个部分。

2.1，wake阶段，认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想像的不一样，改变我的权重使得我想像的东西就是这样的“。

2.2，sleep阶段，生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念“。

由于自动编码器（auto-encoder，即上面说的神经网络。广义上的自动编码器指所有的从低级表示得到高级表示，并能从高级表示生成低级表示的近似的结构，狭义上指的是其中的一种，谷歌的人脸识别用的）有联想功能，也就是缺失部分输入也能得到正确的编码，所以上面说的算法也可以用于有监督学习，训练时y做为顶层网络输入的补充，应用时顶层网络生成y'。

#### 正态分布

正态分布，又叫做高斯分布。

若随机变量$X$，服从一个位置参数为$\mu$、尺度参数为$\sigma$的概率分布，且其概率密度函数为：

$$
f(x)=\frac{1}{\sigma\sqrt{2 \pi} } e^{- \frac{{(x-\mu)^2}}{2\sigma^2}} \tag{1}
$$

则这个随机变量就称为正态随机变量，正态随机变量服从的分布就称为正态分布，记作：

$$
X \sim N(\mu,\sigma^2) \tag{2}
$$

当μ=0,σ=1时，称为标准正态分布：

$$X \sim N(0,1) \tag{3}$$

此时公式简化为：

$$
f(x)=\frac{1}{\sqrt{2 \pi}} e^{- \frac{x^2}{2}} \tag{4}
$$

机器学习领域有个很重要的假设：I.I.D.（独立同分布）假设，就是假设训练数据和测试数据是满足相同分布的，这样就能做到通过训练数据获得的模型能够在测试集获得好的效果。

在深度神经网络中，我们可以将每一层视为对输入的信号做了一次变换：

$$
Z = W \cdot X + B \tag{5}
$$

我们在第5章学过，输入层的数据已经归一化，如果不做归一化，很多时候甚至网络不会收敛，可见归一化的重要性。

随后的网络的每一层的输入数据在经过公式5的运算后，其分布一直在发生变化，前面层训练参数的更新将导致后面层输入数据分布的变化，必然会引起后面每一层输入数据分布的改变，不再是输入的原始数据所适应的分布了。

而且，网络前面几层微小的改变，后面几层就会逐步把这种改变累积放大。训练过程中网络中间层数据分布的改变称之为内部协变量偏移（Internal Covariate Shift）。BN的提出，就是要解决在训练过程中，中间层数据分布发生改变的情况。

比如，在上图中，假设X是服从蓝色或红色曲线的分布，经过公式5后，有可能变成了绿色曲线的分布。

标准正态分布的数值密度占比如图15-11所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/15/bn2.png" ch="500" />

可以看到带来的问题是：

1. 在大于2的区域，激活后的值基本接近1了，饱和输出。如果蓝色曲线表示的数据更偏向右侧的话，激活函数就会失去了作用，因为所有的输出值都是0.94、0.95、0.98这样子的数值，区别不大；
2. 导数数值小，只有不到0.1甚至更小，反向传播的力度很小，网络很难收敛。

四种情况的应对措施：

- 情况1
  
  效果很好，可以考虑进一步降低误差值，提高准确度。

- 情况2

  训练集和验证集同时出现较大的误差，有可能是：迭代次数不够、数据不好、网络设计不好，需要继续训练，观察误差变化情况。

- 情况3

  训练集的误差已经很低了，但验证集误差很高，说明过拟合了，即训练集中的某些特殊样本影响了网络参数，但类似的样本在验证集中并没有出现

- 情况4

  两者误差都很大，目前还看不出来是什么问题，需要继续训练

|符号|含义|
|---|---|
|$x$|测试样本|
|$D$|数据集|
|$y$|x的真实标记|
|$y_D$|x在数据集中标记(可能有误差)|
|$f$|从数据集D学习的模型|
|$f_{x;D}$|从数据集D学习的模型对x的预测输出|
|$f_x$|模型f对x的期望预测输出|
- 偏差：度量了学习算法的期望与真实结果的偏离程度，即学习算法的拟合能力。
- 方差：训练集与验证集的差异造成的模型表现的差异。
- 噪声：当前数据集上任何算法所能到达的泛化误差的下线，即学习问题本身的难度。
均方差损失函数：

$$J(w,b)=\frac{1}{2m}\sum_{i=1}^m (z_i-y_i)^2 + \frac{\lambda}{2m}\sum_{j=1}^n{w_j^2} \tag{5}$$

如果是交叉熵损失函数：

$$J(w,b)= -\frac{1}{m} \sum_{i=1}^m [y_i \ln a_i + (1-y_i) \ln (1-a_i)]+ \frac{\lambda}{2m}\sum_{j=1}^n{w_j^2} \tag{6}$$

#### 拉普拉斯分布

$$
\begin{aligned}
f(x)&=\frac{1}{2b}\exp(-\frac{|x-\mu|}{b})\\\\
&= \frac{1}{2b} \begin{cases} \exp(\frac{x-\mu}{b}), & x \lt \mu \\\\ \exp(\frac{\mu-x}{b}), & x \gt \mu \end{cases}
\end{aligned}
$$
那么参数稀疏有什么好处呢？有两点：

1. 特征选择(Feature Selection)：

    大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，x的大部分元素（也就是特征）都是和最终的输出y没有关系或者不提供任何信息的，在最小化目标函数的时候考虑x这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确y的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。

2. 可解释性(Interpretability)：

    另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：$y=w_1x_1+w_2x_2+…+w_{1000}x_{1000}+b$（当然了，为了让y限定在$[0,1]$的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个$w_i$都非0，医生面对这1000种因素，无法采取针对性治疗。
    #### 要注意的问题

1. 门限值`patience`不能太小，比如小于5，因为很可能在5个`epoch`之外，损失函数值又会再次下降
2. `patience`不能太大，比如大于30，因为在这30个`epoch`之内，由于样本数量少和数据`shuffle`的关系，很可能某个`epoch`的损失函数值会比上一次低，这样忍耐次数计数器`counter`就清零了，从而不能及时停止。
3. 当样本数量少时，为了获得平滑的变化曲线，可以考虑使用加权平均的方式处理当前和历史损失函数值，以避免某一次的高低带来的影响。

### 学习感悟

机器学习技术正在走进数据中心，它既能改善内部IT管理，还能使关键业务流程更加智能化。你可能已经听说过深度学习的神秘性了，它涉及到一切领域，从系统管理到自动驾驶汽车。到底深度学习是一个刚刚在世人面前揭开面纱的非常聪明的新兴人工智能，还是仅仅一种营销宣传手段，将已有的复杂机器学习算法重新包装成为新的卖点？

深度学习无疑激发了大众的想象力，但它其实并不那么复杂。在技术层面上，深度学习主要指大规模运行的大型计算密集型神经网络。这些神经网络往往是由难以用基于逻辑和规则的机器学习方法进行处理的大数据集训练而成，如图像、语音、视频和其他内在具有复杂模式的密集数据。

神经网络本身并不新。几乎从现代计算机开创阶段起，神经网络算法已经被研究用于复杂数据流中辅助识别隐藏的内在模式。在这个意义上，深度学习是建立在众所周知的机器学习技术上的。然而，当新兴计算复杂度更高的神经网络算法与如今的大数据集合应用到一起，创造出了重大的新机遇。使用低成本的云服务或商业scale-out大数据结构，可以创建这些“深度”模型，并实时应用于大规模应用场景中。

敏感的神经网络

神经网络研究起步于上世纪50年代和60年代，最早是为研究人类大脑如何工作而建模出来的。神经网络由多层节点组成，这些节点相互连接组成一张大网，有如大脑中的神经元。每个节点接收输入信号，接下来，它通过一个预先定义好的“激活功能”发出一个输出信号，并传给其他节点，同时确定什么时候节点应该进入活跃状态。简单的，你可以认为节点如何工作取决于其兴奋程度，当一个节点收到一组输入后变得兴奋时，它可以产生一定程度的输出信号，并传递给它的下游节点。有趣的是，一个节点兴奋起来后，它的输出信号可以是正也可以是负；一些节点激活后实际上会抑制另一些节点的兴奋。 节点通过链接互连，每个链接有其自己的权重变量。一个链接的权重会调整经过它传输的信号。神经网络通过逐渐调整其整个网络的链接权重，适应和学习如何识别模式，最终只有被正确识别的模式会产生一个完整的遍布全网络的兴奋传递。

一般情况下，输入数据被格式化为一个输入信号，链接到第一层外部节点。这些节点随后向一个或多个隐藏层发送信号，最后输出层节点发出一个“反馈”给外部世界。由于学习（也即智能）是隐含在链路权重中的，实际应用的核心问题是搞清楚怎么调节或训练所有的链路权重以实现正确模式的应答。今天，神经网络主要通过后向传播的增量学习技术，用在训练数据中寻找正确的模式来完成学习过程。当神经网络生成一种有用的方式识别出正确的样本时，该方法相应的给予链路“奖励”，当神经网络识别出错误的样本时，则给予惩罚。

然而，不可能存在一个能够适用于任何给定问题的神经网络架构。此时机器学习专业知识就是非常重要的了，因为给定一定数量的节点、其激励功能、一定数量的隐藏层以及所有节点的连接关系（例如是密集连接还是稀疏连接，是否存在内部反馈或循环环路），可能存在无数种潜在的神经网络配置方式。传统研究中，受限于硬件条件，神经网络隐藏层的数量设置得很少，即使如此，神经网络已经展现出超过人类的，惊人的和熟练的学习能力。如今，深度学习神经网络可能具有数百层网络，能够完全胜任深度奥妙问题的处理。