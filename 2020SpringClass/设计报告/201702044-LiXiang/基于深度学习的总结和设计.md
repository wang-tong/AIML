# <div align='center' ><font size='70'>åŸºäºæ·±åº¦å­¦ä¹ çš„æ€»ç»“ä¸è®¾è®¡</font></div>

# æ‘˜è¦
æ·±åº¦å­¦ä¹ æ¡†æ¶è¶Šæ¥è¶Šæˆç†Ÿï¼Œå¯¹äºä½¿ç”¨è€…è€Œè¨€å°è£…ç¨‹åº¦è¶Šæ¥è¶Šé«˜ï¼Œå¥½å¤„å°±æ˜¯ç°åœ¨å¯ä»¥éå¸¸å¿«é€Ÿåœ°å°†è¿™äº›æ¡†æ¶ä½œä¸ºå·¥å…·ä½¿ç”¨ï¼Œç”¨éå¸¸å°‘çš„ä»£ç å°±å¯ä»¥æ„å»ºæ¨¡å‹è¿›è¡Œå®éªŒï¼Œåå¤„å°±æ˜¯å¯èƒ½èƒŒååœ°å®ç°éƒ½è¢«éšè—èµ·æ¥äº†ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­å°†å¯¹æ·±åº¦å­¦ä¹ æœ‰å…³çš„çŸ¥è¯†ä¿¡æ¯è¿›è¡Œä¸€ç³»åˆ—çš„æ€»ç»“,è¯¸å¦‚å›¾åƒè¯†åˆ«åŠäººå·¥æ™ºèƒ½çš„æ¦‚æ‹¬,äººè„‘è§†è§‰åŸç†ç­‰çŸ¥è¯†ç‚¹çš„ä»‹ç»,å¹¶è®¾è®¡å’Œå®ç°ä¸€ä¸ªã€è½»é‡çº§çš„ï¼ˆçº¦ 200 è¡Œï¼‰ã€æ˜“äºæ‰©å±•çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ tinynnï¼ˆåŸºäº Python å’Œ Numpy å®ç°ï¼‰.
# æ·±åº¦å­¦ä¹ æ¡†æ¶å®æˆ˜å­¦ä¹ æ€»ç»“1
## å›¾åƒè¯†åˆ«ä¸äººå·¥æ™ºèƒ½

1ã€ä¸»è¦ä»‹ç»å›¾åƒè¯†åˆ«å®šä¹‰å’Œé—®é¢˜

2ã€ä¼ ç»Ÿå›¾åƒè¯†åˆ«æ–¹æ³•

3ã€äººå·¥æ™ºèƒ½å‘å±•å†ç¨‹

### å›¾åƒè¯†åˆ«

1ã€é—®é¢˜ï¼š
è¯­ä¹‰é¸¿æ²Ÿsemantic gapã€‚å›¾åƒçš„åº•å±‚è§†è§‰ç‰¹æ€§å’Œé«˜å±‚è¯­ä¹‰æ¦‚å¿µä¹‹é—´çš„é¸¿æ²Ÿã€‚ç›¸ä¼¼çš„è§†è§‰ç‰¹æ€§(color, texture, shapeï¼Œâ€¦) ï¼Œä¸åŒçš„è¯­ä¹‰æ¦‚å¿µã€‚ä¸ç›¸ä¼¼çš„è§†è§‰ç‰¹æ€§ï¼Œç›¸åŒçš„è¯­ä¹‰æ¦‚å¿µã€‚

2ã€ç›®æ ‡ï¼š
äººä»¬çœ‹åˆ°çš„æ˜¯å›¾ç‰‡å´çœ‹ä¸åˆ°åé¢äºŒç»´åƒç´  å› æ­¤å°†å›¾åƒç”¨æ•°å­¦çŸ©é˜µè¡¨è¾¾çš„å€¼æ¥æ›¿ä»£ï¼Œæ›´èƒ½è®©äººä»¬èƒ½å¤Ÿç†è§£
![](/media.png)
### ä¼ ç»Ÿå›¾åƒè¯†åˆ«æ–¹æ³•
1ã€å›¾åƒè¯†åˆ«åŸºæœ¬æ¡†æ¶
æµ‹é‡ç©ºé—´â€”â€”â€”â€”ç‰¹å¾ç©ºé—´â€”â€”â€”â€”ç±»åˆ«ç©ºé—´
â€‹![](/media1.png)

2ã€ä¼ ç»Ÿå›¾åƒè¯†åˆ«æŠ€æœ¯

1ã€æ—©æœŸå›¾åƒè¯†åˆ«æŠ€æœ¯(1990-2003)

æµç¨‹ï¼š

ç‰¹å¾æå–â€”â€”â€”â€”â€”â€”ç´¢å¼•æŠ€æœ¯â€”â€”â€”â€”â€”â€”ç›¸å…³åé¦ˆâ€”â€”â€”â€”â€”â€”é‡æ’åº

å…·ä½“æ¥è®²å°±æ˜¯ï¼šé€šè¿‡äº’è”ç½‘æˆ–å…¶ä»–ç›¸æœºè®¾å¤‡å°†é‡‡é›†åˆ°çš„å›¾åƒå­˜åˆ°å°å‹æ•°æ®åº“ï¼Œç„¶åé€šè¿‡è®¡ç®—æœºç›¸å…³è½¯ä»¶å¯¹éœ€è¦å›¾åƒå¤„ç†çš„å›¾åƒæå–ç‰¹å¾ï¼Œç„¶åè¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œæœ€åå°†ç»“æœè¿”å›ã€‚

ç‰¹å¾æå–æ–¹æ³•ï¼š

1ã€å…¨å±€ç‰¹å¾æå–ï¼šæ¯”å¦‚é¢œè‰²ã€å½¢çŠ¶ã€çº¹ç†å°†ä»–ä»¬å˜æˆç‰¹å¾å‘é‡ã€‚å…¨å±€æå‡ºå¿½ç•¥äº†ç»†èŠ‚ã€‚
åŸå›¾ç‰‡åˆ°å‘é‡ç©ºé—´æ˜ å°„å†åˆ°å‘é‡è¡¨ç¤ºã€‚

ç‰¹å¾å˜æ¢ï¼š

2ã€æ˜ å°„ä¸ºä½ç»´ç©ºé—´ä¸‹çš„å‘é‡è¡¨ç¤ºï¼Œå¸¸ç”¨æ–¹æ³•å¦‚PCAã€MDSã€ISOMAPã€LLEã€Laplacian Eigenmap  

ç©ºé—´å˜æ¢ï¼šç›¸ä¼¼çš„ç‰©ä½“è·ç¦»è¿‘, ä¸ç›¸ä¼¼çš„ç‰©ä½“è·ç¦»è¿œ

ç´¢å¼•æŠ€æœ¯ç¤ºä¾‹ï¼šäºŒè¿›åˆ¶å“ˆå¸Œ  

ç›¸å…³åé¦ˆ
Explicit feedback ï¼šåé¦ˆæ­£ä¾‹æˆ–è€…è´Ÿä¾‹
Implicit feedback: æ ¹æ®å¯è§‚å¯Ÿçš„è¡Œä¸ºæ¨æ–­ç”¨æˆ·æ„å›¾

2ã€ä¸­æœŸå›¾åƒè¯†åˆ«æŠ€æœ¯(2003-2012)

æµç¨‹ï¼š

ç‰¹å¾æå–â€”â€”â€”â€”â€”â€”å‘é‡åŒ–â€”â€”â€”â€”â€”â€”ç´¢å¼•æŠ€æœ¯â€”â€”â€”â€”â€”â€”åå¤„ç†  

æ–‡æœ¬æœç´¢çš„ç»å…¸æ¨¡å‹ï¼šè¯è¢‹æ¨¡å‹ï¼ˆBag-of-Words ï¼‰

ç‰¹å¾æå–æ–¹æ³•ï¼š

1ã€å±€éƒ¨ç‰¹å¾æå–ï¼š

ç‰¹å¾æ£€æµ‹å­Feature Detector
æ£€æµ‹å›¾åƒåŒºå—ä¸­å¿ƒä½ç½®ï¼šHarris, DoG,SURF,Harris-Affine  

ç‰¹å¾æè¿°å­Feature Descriptorï¼š
æè¿°åŒºå—çš„è§†è§‰å†…å®¹ï¼šSIFT,GLOH,Shape,Context,ORB

### äººå·¥æ™ºèƒ½å‘å±•å†ç¨‹

1ã€æ¨ç†æœŸï¼ˆ20ä¸–çºª50-70å¹´ä»£åˆï¼‰ è®¤ä¸ºåªè¦ç»™æœºå™¨èµ‹äºˆé€»è¾‘æ¨ç†èƒ½åŠ›  

2ã€çŸ¥è¯†æœŸï¼ˆ20ä¸–çºª70å¹´ä»£ä¸­æœŸï¼‰ è®¤ä¸ºè¦ä½¿æœºå™¨å…·æœ‰æ™ºèƒ½ï¼Œå°±å¿…é¡»è®¾æ³•ä½¿æœºå™¨æ‹¥æœ‰çŸ¥è¯†  

3ã€å­¦ç§‘å½¢æˆï¼ˆ20ä¸–çºª80å¹´ä»£ï¼‰ æœºå™¨å­¦ä¹ æˆä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘é¢†åŸŸ  

4ã€ç¹è£æœŸï¼ˆ20ä¸–çºª80å¹´ä»£-è‡³ä»Šï¼‰ 20 ä¸–çºª90 å¹´ä»£åï¼Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•å ä¸»å¯¼ï¼Œä»£è¡¨ä¸ºSVM 2006 è‡³ä»Šï¼Œå¤§æ•°æ®åˆ†æçš„éœ€æ±‚ï¼Œç¥ç»ç½‘ç»œåˆè¢«é‡è§†ï¼Œæˆä¸ºæ·±åº¦å­¦ä¹ ç†è®ºçš„åŸºç¡€.

### æœºå™¨å­¦ä¹ 

1ã€æœºå™¨å­¦ä¹ æ¨¡å‹
![](/media2.png)

2ã€æœºå™¨å­¦ä¹ ä¸€èˆ¬è¿‡ç¨‹  
å®šä¹‰è®­ç»ƒæ•°æ®â€”â€”â€”â€”â€”â€”ç„¶åé€šè¿‡è®¾è®¡çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒâ€”â€”â€”â€”â€”â€”â€”â€”ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œæµ‹è¯•â€”â€”â€”â€”â€”â€”â€”â€”æœ€åè·å¾—å­¦ä¹ ç»“æœç”¨æ¥é¢„æµ‹

### æ·±åº¦å­¦ä¹ 

1ã€æ·±åº¦å­¦ä¹ æ¨¡å‹

ä¼ ç»Ÿæ–¹æ³•ä¸æ·±åº¦å­¦ä¹ åœ¨ç‰¹å¾ç©ºé—´è¡¨è¾¾æ–¹å¼ä¸åŒ  
ä¼ ç»Ÿæ–¹æ³•ï¼šäººå·¥ç‰¹å¾æå–+åˆ†ç±»å™¨=è®¾è®¡ç‰¹å¾  
æ·±åº¦å­¦ä¹ ï¼šå­¦ä¹ ç‰¹å¾
![](media3.png)

### å°ç»“1

ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰ä¸æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ åœ¨å¯¹å›¾åƒå¤„ç†ä¸­ç‰¹å¾ç©ºé—´è¡¨è¾¾æ–¹å¼ä¸åŒï¼Œæ·±åº¦å­¦ä¹ åœ¨äºå­¦ä¹ ç‰¹å¾ï¼ŒäºŒä¼ ç»Ÿæ–¹æ³•åœ¨äºè®¾è®¡ç‰¹å¾ã€‚å…¶ä¸­æˆ‘ä»¬éœ€è¦åŸºæœ¬æ•°å­¦çŸ¥è¯†å’Œpythonè¯­è¨€çŸ¥è¯†,ç†Ÿæ‚‰numpyåº“çš„æ“ä½œä»¥åŠåç»­éœ€è¦å¯¹å„ç§æ·±åº¦å­¦ä¹ ç®—æ³•çš„äº†è§£ä¸å¤ç°çŸ¥è¯†ã€‚

# æ·±åº¦å­¦ä¹ æ¡†æ¶å®æˆ˜å­¦ä¹ æ€»ç»“2
## æ·±åº¦å­¦ä¹ 
### äººè„‘è§†è§‰åŸç†
![](media4.png)
### ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰çš„ç›®çš„ï¼šå¯»æ‰¾ä¸€ä¸ªåˆé€‚çš„å‡½æ•°f(x)
![](media5.png)

ä¾‹å¦‚ï¼š

æ‰‹å†™æ•°å­—è¯†åˆ«ä»»åŠ¡â€”â€”å›¾åƒè¯†åˆ«

çŒ«ç‹—åˆ†ç±»ä»»åŠ¡â€”â€”å›¾åƒåˆ†ç±»

éƒ½æ˜¯éœ€è¦æ‰¾åˆ°åˆé€‚çš„å‡½æ•°æ¨¡å‹å»è¡¨å¾å›¾åƒã€‚

*æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­ äººç¡®å®å‡½æ•°é›†åˆ äººè¯„ä»·å¥½å æœºå™¨æ‰¾å‡ºæœ€å¥½çš„å‡½æ•°æ¨¡å‹*

ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰

äººâ€”â€”â€”â€”â€”â€”â€”â€”å»ºç«‹æ¨¡å‹ {ğ‘“(ğ‘¥)=ğ‘˜ğ‘¥|ğ‘˜=1,2,3â€¦â€¦}

äººâ€”â€”â€”â€”â€”â€”â€”â€”æŸå¤±å‡½æ•° ğ‘šğ‘–ğ‘›2(ğ‘¦âˆ’ğ‘“(ğ‘¥))

æœºå™¨â€”â€”â€”â€”â€”â€”å‚æ•°å­¦ä¹  ğ‘“(ğ‘¥)=2ğ‘¥
![](media6.png)

### æ·±åº¦å­¦ä¹ æ­¥éª¤
1ã€å»ºç«‹æ¨¡å‹

1ï¼‰ã€é€‰æ‹©ä»€ä¹ˆæ ·çš„ç½‘ç»œç»“æ„

2ï¼‰ã€å¤šå°‘å±‚åˆé€‚ éšè—å±‚è¶Šæ·±ç½‘ç»œè¶Šæ·±

*æ¿€æ´»å‡½æ•°ï¼š
å»ºç«‹æ¨¡å‹éœ€è¦æ¿€æ´»å‡½æ•°ï¼Œå¼•å…¥æ¿€æ´»å‡½æ•°å¢å¼ºç½‘ç»œè¡¨è¾¾èƒ½åŠ›ï¼Œå°†çº¿æ€§å‡½æ•°->éçº¿æ€§å‡½æ•° éçº¿æ€§çš„æ¿€æ´»å‡½æ•°éœ€è¦æœ‰è¿ç»­æ€§ã€‚å› ä¸ºè¿ç»­éçº¿æ€§æ¿€æ´»å‡½æ•°å¯ä»¥å¯å¯¼çš„ï¼Œæ‰€ä»¥å¯ä»¥ç”¨æœ€ä¼˜åŒ–çš„æ–¹æ³•æ¥æ±‚è§£.*

å‰é¦ˆç¥ç»ç½‘ç»œ

å‰å‘ä¼ æ’­

2ã€æŸå¤±å‡½æ•°

1ï¼‰ã€å¹³æ–¹å·®æŸå¤±å‡½æ•°

2ï¼‰ã€äº¤å‰ç†µæŸå¤±å‡½æ•°

æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„å‡½æ•°ä½¿å¾—æŸå¤±å‡½æ•°lossè¶Šå°è¶Šå¥½

3ã€å‚æ•°å­¦ä¹ 

1ï¼‰ã€æ¢¯åº¦ä¸‹é™

2ï¼‰ã€åå‘ä¼ æ’­ç®—æ³•

w,bå‚æ•°æ‰¾åˆ°ä¸€ç»„åˆé€‚çš„æƒé‡å’Œåç½®å‚æ•°ä½¿å¾—æ€»æŸå¤±æœ€å°

æ¢¯åº¦ä¸ºè´Ÿâ€”â€”â€”â€”â€”â€”å¢åŠ w

æ¢¯åº¦ä¸ºæ­£â€”â€”â€”â€”â€”â€”å‡å°w

ğ‘¤âˆ’ğœ‚ğœ•ğ¿ğœ•ğ‘¤â€”â€”â€”â€”â€”â€”ğœ‚ä¸ºå­¦ä¹ ç‡

å±€éƒ¨æœ€ä¼˜ä¸å…¨å±€æœ€ä¼˜

![](media7.png)

### å…¨è¿æ¥ç¥ç»ç½‘ç»œçš„é—®é¢˜
* æ¨¡å‹ç»“æ„ä¸çµæ´»
  
å¯¹äº100100çš„è¯æ¯”1616çš„å›¾ç‰‡æ‰‹å†™åˆ†ç±»ä»»åŠ¡ä½¿ç”¨å…¨è¿æ¥ç½‘ç»œå‚æ•°
è¦å¤šå¥½å¤šï¼ˆå¢åŠ æ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°æˆ–è€…å¢åŠ ç½‘ç»œçš„å±‚æ•°ï¼‰

* æ¨¡å‹å‚æ•°å¤ªå¤š
  
è¾“å…¥ä¸º16 x 16 çš„å›¾ç‰‡ï¼Œè¾“å…¥å±‚ä¸º256ä¸ªç¥ç»å…ƒï¼Œéšè—å±‚æ¯å±‚1000ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚10ä¸ªã€‚å‡è®¾å…±5å±‚ï¼Œåˆ™å…±éœ€è¦å­¦ä¹ ï¼ˆ256*103+106+106+104 ï¼‰ä¸ªwå†åŠ ï¼ˆ1000+1000+1000+10ï¼‰ä¸ªb

### å·ç§¯ç¥ç»ç½‘ç»œ

1ã€å»ºç«‹æ¨¡å‹

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

å·ç§¯å±‚

Poolingå±‚

å·ç§¯ç¥ç»ç½‘ç»œåº”ç”¨ç¤ºä¾‹

å‡å°‘ç½‘ç»œå‚æ•°ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼š

å±€éƒ¨è¿æ¥ï¼ˆroiï¼‰ã€åªæœ‰ç¬¬ä¸€å±‚æ˜¯åœ¨è¾“å…¥å›¾åƒä¸Šè®¡ç®—ï¼Œåé¢æ˜¯åœ¨ç‰¹å¾å›¾ä¸Šè®¡ç®—ã€‚

æƒé‡å…±äº«(ç¥ç»å…ƒæƒé‡ç›¸åŒï¼Œå‡ä¸ºğ‘¤1,â€¦,ğ‘¤n)ã€
ä¸‹é‡‡æ ·(å¯¹å›¾åƒåƒç´ è¿›è¡Œä¸‹é‡‡æ ·ï¼Œå¹¶ä¸ä¼šå¯¹ç‰©ä½“è¿›è¡Œæ”¹å˜è™½ç„¶ä¸‹é‡‡æ ·ä¹‹åçš„å›¾åƒå°ºå¯¸å˜å°äº†ï¼Œä½†æ˜¯å¹¶ä¸å½±å“æˆ‘ä»¬å¯¹å›¾åƒä¸­ç‰©ä½“çš„è¯†åˆ«)
![](media8.png)
![](media9.png)
![](media10.png)
![](media11.png)

2ã€æŸå¤±å‡½æ•°

åˆ†ç±»æŸå¤±ï¼šäº¤å‰ç†µæŸå¤±å‡½æ•°

å›å½’æŸå¤±ï¼šå¹³æ–¹æŸå¤±å‡½æ•°

3ã€å‚æ•°å­¦ä¹ 

1ï¼‰ã€æ¢¯åº¦ä¸‹é™

2ï¼‰ã€åå‘ä¼ æ’­ç®—æ³•

w,bå‚æ•°æ‰¾åˆ°ä¸€ç»„åˆé€‚çš„æƒé‡å’Œåç½®å‚æ•°ä½¿å¾—æ€»æŸå¤±æœ€å°

æ¢¯åº¦ä¸ºè´Ÿâ€”â€”â€”â€”â€”â€”å¢åŠ w

æ¢¯åº¦ä¸ºæ­£â€”â€”â€”â€”â€”â€”å‡å°w

ğ‘¤âˆ’ğœ‚ğœ•ğ¿ğœ•ğ‘¤â€”â€”â€”â€”â€”â€”ğœ‚ä¸ºå­¦ä¹ ç‡

å±€éƒ¨æœ€ä¼˜ä¸å…¨å±€æœ€ä¼˜

### å…¨è¿æ¥ç¥ç»ç½‘ç»œçš„é—®é¢˜vså·ç§¯ç¥ç»ç½‘ç»œ
![](media12.png)

*ç»å…¸CNN
![](media13.png)

å°ç»“2
1ã€é¦–å…ˆæ˜¯ä½¿ç”¨ä½•ç§ç½‘ç»œï¼Œå“ªå‡ ç§ç½‘ç»œç»“åˆä¸€èµ·ï¼Œå¤šå°‘å±‚åˆé€‚å¹¶ä¸”æ¯ä¸€å±‚è¾“å…¥é€šé“ å·ç§¯æ ¸å¤§å° å·ç§¯æ ¸ä¸ªæ•° è¾“å‡ºé€šé“éƒ½è¦ä¼šè®¡ç®—

2ã€å…¶æ¬¡ç½‘ç»œè°ƒå‚æ–¹æ³• å­¦ä¹ ä½¿ç”¨ç½‘ç»œè°ƒä¼˜æ–¹æ³•å’Œç»éªŒ

3ã€æœ€åè¦å°†ç®—æ³•å’Œå®é™…ç›¸ç»“åˆã€‚

# åŸºäºæ·±åº¦å­¦ä¹ çš„MNISTå®ç°
## ç»„ä»¶æŠ½è±¡

é¦–å…ˆè€ƒè™‘ç¥ç»ç½‘ç»œè¿ç®—çš„æµç¨‹ï¼Œç¥ç»ç½‘ç»œè¿ç®—ä¸»è¦åŒ…å«è®­ç»ƒ training å’Œé¢„æµ‹ predict ï¼ˆæˆ– inferenceï¼‰ ä¸¤ä¸ªé˜¶æ®µï¼Œè®­ç»ƒçš„åŸºæœ¬æµç¨‹æ˜¯ï¼šè¾“å…¥æ•°æ® -> ç½‘ç»œå±‚å‰å‘ä¼ æ’­ -> è®¡ç®—æŸå¤± -> ç½‘ç»œå±‚åå‘ä¼ æ’­æ¢¯åº¦ -> æ›´æ–°å‚æ•°ï¼Œé¢„æµ‹çš„åŸºæœ¬æµç¨‹æ˜¯ è¾“å…¥æ•°æ® -> ç½‘ç»œå±‚å‰å‘ä¼ æ’­ -> è¾“å‡ºç»“æœã€‚ä»è¿ç®—çš„è§’åº¦çœ‹ï¼Œä¸»è¦å¯ä»¥åˆ†ä¸ºä¸‰ç§ç±»å‹çš„è®¡ç®—ï¼š

1ã€æ•°æ®åœ¨ç½‘ç»œå±‚ä¹‹é—´çš„æµåŠ¨ï¼šå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­å¯ä»¥çœ‹åšæ˜¯å¼ é‡ Tensorï¼ˆå¤šç»´æ•°ç»„ï¼‰åœ¨ç½‘ç»œå±‚ä¹‹é—´çš„æµåŠ¨ï¼ˆå‰å‘ä¼ æ’­æµåŠ¨çš„æ˜¯è¾“å…¥è¾“å‡ºï¼Œåå‘ä¼ æ’­æµåŠ¨çš„æ˜¯æ¢¯åº¦ï¼‰ï¼Œæ¯ä¸ªç½‘ç»œå±‚ä¼šè¿›è¡Œä¸€å®šçš„è¿ç®—ï¼Œç„¶åå°†ç»“æœè¾“å…¥ç»™ä¸‹ä¸€å±‚ã€‚

2ã€è®¡ç®—æŸå¤±ï¼šè¡”æ¥å‰å‘å’Œåå‘ä¼ æ’­çš„ä¸­é—´è¿‡ç¨‹ï¼Œå®šä¹‰äº†æ¨¡å‹çš„è¾“å‡ºä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œç”¨æ¥åç»­æä¾›åå‘ä¼ æ’­æ‰€éœ€çš„ä¿¡æ¯ã€‚

3ã€å‚æ•°æ›´æ–°ï¼šä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦å¯¹ç½‘ç»œå‚æ•°è¿›è¡Œæ›´æ–°çš„ä¸€ç±»è®¡ç®—ã€‚

åŸºäºè¿™ä¸ªä¸‰ç§ç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ç½‘ç»œçš„åŸºæœ¬ç»„ä»¶åšä¸€ä¸ªæŠ½è±¡

* tensor å¼ é‡ï¼Œè¿™ä¸ªæ˜¯ç¥ç»ç½‘ç»œä¸­æ•°æ®çš„åŸºæœ¬å•ä½
* layers ç½‘ç»œå±‚ï¼Œè´Ÿè´£æ¥æ”¶ä¸Šä¸€å±‚çš„è¾“å…¥ï¼Œè¿›è¡Œè¯¥å±‚çš„è¿ç®—ï¼Œå°†ç»“æœè¾“å‡ºç»™ä¸‹ä¸€å±‚ï¼Œç”±äº tensor çš„æµåŠ¨æœ‰å‰å‘å’Œåå‘ä¸¤ä¸ªæ–¹å‘ï¼Œå› æ­¤å¯¹äºæ¯ç§ç±»å‹ç½‘ç»œå±‚æˆ‘ä»¬éƒ½éœ€è¦åŒæ—¶å®ç° forward å’Œ backward ä¸¤ç§è¿ç®—
* losses æŸå¤±ï¼Œåœ¨ç»™å®šæ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹åï¼Œè¯¥ç»„ä»¶è¾“å‡ºæŸå¤±å€¼ä»¥åŠå…³äºæœ€åä¸€å±‚çš„æ¢¯åº¦ï¼ˆç”¨äºæ¢¯åº¦å›ä¼ ï¼‰
* optimizer ä¼˜åŒ–å™¨ï¼Œè´Ÿè´£ä½¿ç”¨æ¢¯åº¦æ›´æ–°æ¨¡å‹çš„å‚æ•°
ç„¶åæˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›ç»„ä»¶æŠŠä¸Šé¢è¿™ä¸ª 4 ç§åŸºæœ¬ç»„ä»¶æ•´åˆåˆ°ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ª pipeline

* nn ç»„ä»¶è´Ÿè´£ç®¡ç† tensor åœ¨ layers ä¹‹é—´çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼ŒåŒæ—¶èƒ½æä¾›è·å–å‚æ•°ã€è®¾ç½®å‚æ•°ã€è·å–æ¢¯åº¦çš„æ¥å£
* model ç»„ä»¶è´Ÿè´£æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œå½¢æˆæ•´ä¸ª pipelineã€‚å³ nn ç»„ä»¶è¿›è¡Œå‰å‘ä¼ æ’­ -> losses ç»„ä»¶è®¡ç®—æŸå¤±å’Œæ¢¯åº¦ -> nn ç»„ä»¶å°†æ¢¯åº¦åå‘ä¼ æ’­ -> optimizer ç»„ä»¶å°†æ¢¯åº¦æ›´æ–°åˆ°å‚æ•°ã€‚
  
åŸºæœ¬çš„æ¡†æ¶å›¾å¦‚ä¸‹å›¾
![](media.jpg)

## ç»„ä»¶å®ç°

æŒ‰ç…§ä¸Šé¢çš„æŠ½è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºæ•´ä¸ªæµç¨‹ä»£ç å¦‚ä¸‹ã€‚

```
 # define model
net = Net([layer1, layer2, ...])
model = Model(net, loss_fn, optimizer)
```

```
# training
pred = model.forward(train_X)
loss, grads = model.backward(pred, train_Y)
model.apply_grad(grads)
```

```
# inference
test_pred = model.forward(test_X)
```

é¦–å…ˆå®šä¹‰ netï¼Œnet çš„è¾“å…¥æ˜¯å¤šä¸ªç½‘ç»œå±‚ï¼Œç„¶åå°† netã€lossã€optimizer ä¸€èµ·ä¼ ç»™ modelã€‚model å®ç°äº† forwardã€backward å’Œ apply_grad ä¸‰ä¸ªæ¥å£åˆ†åˆ«å¯¹åº”å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ä¸‰ä¸ªåŠŸèƒ½ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹è¿™é‡Œè¾¹å„ä¸ªéƒ¨åˆ†åˆ†åˆ«å¦‚ä½•å®ç°ã€‚

* tensor
tensor å¼ é‡æ˜¯ç¥ç»ç½‘ç»œä¸­åŸºæœ¬çš„æ•°æ®å•ä½ï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥ä½¿ç”¨ numpy.ndarray ç±»ä½œä¸º tensor ç±»çš„å®ç°

* layers
ä¸Šé¢æµç¨‹ä»£ç ä¸­ model è¿›è¡Œ forward å’Œ backwardï¼Œå…¶å®åº•å±‚éƒ½æ˜¯ç½‘ç»œå±‚åœ¨è¿›è¡Œå®é™…è¿ç®—ï¼Œå› æ­¤ç½‘ç»œå±‚éœ€è¦æœ‰æä¾› forward å’Œ backward æ¥å£è¿›è¡Œå¯¹åº”çš„è¿ç®—ã€‚åŒæ—¶è¿˜åº”è¯¥å°†è¯¥å±‚çš„å‚æ•°å’Œæ¢¯åº¦è®°å½•ä¸‹æ¥ã€‚å…ˆå®ç°ä¸€ä¸ªåŸºç±»å¦‚ä¸‹

```
# layers.py
class Layer(object):
    def __init__(self, name):
        self.name = name
        self.params, self.grads = None, None

    def forward(self, inputs):
        raise NotImplementedError

    def backward(self, grad):
        raise NotImplementedError
```

æœ€åŸºç¡€çš„ä¸€ç§ç½‘ç»œå±‚æ˜¯å…¨è¿æ¥ç½‘ç»œå±‚ï¼Œå®ç°å¦‚ä¸‹ã€‚forward æ–¹æ³•æ¥æ”¶ä¸Šå±‚çš„è¾“å…¥ inputsï¼Œå®ç°Ï‰x+bçš„è¿ç®—ï¼›backward çš„æ–¹æ³•æ¥æ”¶æ¥è‡ªä¸Šå±‚çš„æ¢¯åº¦ï¼Œè®¡ç®—å…³äºå‚æ•° Ï‰ å’Œ b è¾“å…¥çš„æ¢¯åº¦ï¼Œç„¶åè¿”å›å…³äºè¾“å…¥çš„æ¢¯åº¦ã€‚

```
# layers.py
class Dense(Layer):
    def __init__(self, num_in, num_out,
                 w_init=XavierUniformInit(),
                 b_init=ZerosInit()):
        super().__init__("Linear")

        self.params = {
            "w": w_init([num_in, num_out]),
            "b": b_init([1, num_out])}

        self.inputs = None

    def forward(self, inputs):
        self.inputs = inputs
        return inputs @ self.params["w"] + self.params["b"]

    def backward(self, grad):
        self.grads["w"] = self.inputs.T @ grad
        self.grads["b"] = np.sum(grad, axis=0)
        return grad @ self.params["w"].T
```

åŒæ—¶ç¥ç»ç½‘ç»œä¸­çš„å¦ä¸€ä¸ªé‡è¦çš„éƒ¨åˆ†æ˜¯æ¿€æ´»å‡½æ•°ã€‚æ¿€æ´»å‡½æ•°å¯ä»¥çœ‹åšæ˜¯ä¸€ç§ç½‘ç»œå±‚ï¼ŒåŒæ ·éœ€è¦å®ç° forward å’Œ backward æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ç»§æ‰¿ Layer ç±»å®ç°æ¿€æ´»å‡½æ•°ç±»ï¼Œè¿™é‡Œå®ç°äº†æœ€å¸¸ç”¨çš„ ReLU æ¿€æ´»å‡½æ•°ã€‚func å’Œ derivation_func æ–¹æ³•åˆ†åˆ«å®ç°å¯¹åº”æ¿€æ´»å‡½æ•°çš„æ­£å‘è®¡ç®—å’Œæ¢¯åº¦è®¡ç®—ã€‚

```
# layers.py
class Activation(Layer):
    """Base activation layer"""
    def __init__(self, name):
        super().__init__(name)
        self.inputs = None

    def forward(self, inputs):
        self.inputs = inputs
        return self.func(inputs)

    def backward(self, grad):
        return self.derivative_func(self.inputs) * grad

    def func(self, x):
        raise NotImplementedError

    def derivative_func(self, x):
        raise NotImplementedError


class ReLU(Activation):
    """ReLU activation function"""
    def __init__(self):
        super().__init__("ReLU")

    def func(self, x):
        return np.maximum(x, 0.0)

    def derivative_func(self, x):
        return x > 0.0
```
* nn
  
ä¸Šæ–‡æåˆ° nn ç±»è´Ÿè´£ç®¡ç† tensor åœ¨ layers ä¹‹é—´çš„å‰å‘å’Œåå‘ä¼ æ’­ã€‚forward æ–¹æ³•å¾ˆç®€å•ï¼ŒæŒ‰é¡ºåºéå†æ‰€æœ‰å±‚ï¼Œæ¯å±‚è®¡ç®—çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼›backward åˆ™é€†åºéå†æ‰€æœ‰å±‚ï¼Œå°†æ¯å±‚çš„æ¢¯åº¦ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚è¿™é‡Œæˆ‘ä»¬è¿˜å°†æ¯ä¸ªç½‘ç»œå±‚å‚æ•°çš„æ¢¯åº¦ä¿å­˜ä¸‹æ¥è¿”å›ï¼Œåé¢å‚æ•°æ›´æ–°éœ€è¦ç”¨åˆ°ã€‚å¦å¤– nn ç±»è¿˜å®ç°äº†è·å–å‚æ•°ã€è®¾ç½®å‚æ•°ã€è·å–æ¢¯åº¦çš„æ¥å£ï¼Œä¹Ÿæ˜¯åé¢å‚æ•°æ›´æ–°æ—¶éœ€è¦ç”¨åˆ°

```
# nn.py
class Net(object):
    def __init__(self, layers):
        self.layers = layers

    def forward(self, inputs):
        for layer in self.layers:
            inputs = layer.forward(inputs)
        return inputs

    def backward(self, grad):
        all_grads = []
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
            all_grads.append(layer.grads)
        return all_grads[::-1]

    def get_params_and_grads(self):
        for layer in self.layers:
            yield layer.params, layer.grads

    def get_parameters(self):
        return [layer.params for layer in self.layers]

    def set_parameters(self, params):
        for i, layer in enumerate(self.layers):
            for key in layer.params.keys():
                layer.params[key] = params[i][key]
```
* losses
  
ä¸Šæ–‡æˆ‘ä»¬æåˆ° losses ç»„ä»¶éœ€è¦åšä¸¤ä»¶äº‹æƒ…ï¼Œç»™å®šäº†é¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼Œéœ€è¦è®¡ç®—æŸå¤±å€¼å’Œå…³äºé¢„æµ‹å€¼çš„æ¢¯åº¦ã€‚æˆ‘ä»¬åˆ†åˆ«å®ç°ä¸º loss å’Œ grad ä¸¤ä¸ªæ–¹æ³•ï¼Œè¿™é‡Œæˆ‘ä»¬å®ç°å¤šåˆ†ç±»å›å½’å¸¸ç”¨çš„ SoftmaxCrossEntropyLoss æŸå¤±ã€‚

æ¢¯åº¦ç¨å¾®å¤æ‚ä¸€ç‚¹ï¼Œç›®æ ‡ç±»åˆ«å’Œéç›®æ ‡ç±»åˆ«çš„è®¡ç®—å…¬å¼ä¸åŒã€‚å¯¹äºç›®æ ‡ç±»åˆ«ç»´åº¦ï¼Œå…¶æ¢¯åº¦ä¸ºå¯¹åº”ç»´åº¦æ¨¡å‹è¾“å‡ºæ¦‚ç‡å‡ä¸€ï¼Œå¯¹äºéç›®æ ‡ç±»åˆ«ç»´åº¦ï¼Œå…¶æ¢¯åº¦ä¸ºå¯¹åº”ç»´åº¦è¾“å‡ºæ¦‚ç‡æœ¬èº«ã€‚![](media14.png)

ä»£ç å®ç°å¦‚ä¸‹

```
# losses.py
class BaseLoss(object):
    def loss(self, predicted, actual):
        raise NotImplementedError

    def grad(self, predicted, actual):
        raise NotImplementedError


class CrossEntropyLoss(BaseLoss):
    def loss(self, predicted, actual):
        m = predicted.shape[0]
        exps = np.exp(predicted - np.max(predicted, axis=1, keepdims=True))
        p = exps / np.sum(exps, axis=1, keepdims=True)
        nll = -np.log(np.sum(p * actual, axis=1))
        return np.sum(nll) / m

    def grad(self, predicted, actual):
        m = predicted.shape[0]
        grad = np.copy(predicted)
        grad -= actual
        return grad / m
```
* optimizer
  
optimizer ä¸»è¦å®ç°ä¸€ä¸ªæ¥å£ compute_stepï¼Œè¿™ä¸ªæ–¹æ³•æ ¹æ®å½“å‰çš„æ¢¯åº¦ï¼Œè®¡ç®—è¿”å›å®é™…ä¼˜åŒ–æ—¶æ¯ä¸ªå‚æ•°æ”¹å˜çš„æ­¥é•¿ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå®ç°å¸¸ç”¨çš„ Adam ä¼˜åŒ–å™¨ã€‚

```
# optimizer.py
class BaseOptimizer(object):
    def __init__(self, lr, weight_decay):
        self.lr = lr
        self.weight_decay = weight_decay

    def compute_step(self, grads, params):
        step = list()
        # flatten all gradients
        flatten_grads = np.concatenate(
            [np.ravel(v) for grad in grads for v in grad.values()])
        # compute step
        flatten_step = self._compute_step(flatten_grads)
        # reshape gradients
        p = 0
        for param in params:
            layer = dict()
            for k, v in param.items():
                block = np.prod(v.shape)
                _step = flatten_step[p:p+block].reshape(v.shape)
                _step -= self.weight_decay * v
                layer[k] = _step
                p += block
            step.append(layer)
        return step

    def _compute_step(self, grad):
        raise NotImplementedError

class Adam(BaseOptimizer):
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999,
                 eps=1e-8, weight_decay=0.0):
        super().__init__(lr, weight_decay)
        self._b1, self._b2 = beta1, beta2
        self._eps = eps

        self._t = 0
        self._m, self._v = 0, 0

    def _compute_step(self, grad):
        self._t += 1
        self._m = self._b1 * self._m + (1 - self._b1) * grad
        self._v = self._b2 * self._v + (1 - self._b2) * (grad ** 2)
        # bias correction
        _m = self._m / (1 - self._b1 ** self._t)
        _v = self._v / (1 - self._b2 ** self._t)
        return -self.lr * _m / (_v ** 0.5 + self._eps)
```
* model
  
æœ€å model ç±»å®ç°äº†æˆ‘ä»¬ä¸€å¼€å§‹è®¾è®¡çš„ä¸‰ä¸ªæ¥å£ forwardã€backward å’Œ apply_grad ï¼Œforward ç›´æ¥è°ƒç”¨ net çš„ forward ï¼Œbackward ä¸­æŠŠ net ã€lossã€optimizer ä¸²èµ·æ¥ï¼Œå…ˆè®¡ç®—æŸå¤± lossï¼Œç„¶ååå‘ä¼ æ’­å¾—åˆ°æ¢¯åº¦ï¼Œç„¶å optimizer è®¡ç®—æ­¥é•¿ï¼Œæœ€åç”± apply_grad å¯¹å‚æ•°è¿›è¡Œæ›´æ–°

```
# model.py
class Model(object):
    def __init__(self, net, loss, optimizer):
        self.net = net
        self.loss = loss
        self.optimizer = optimizer

    def forward(self, inputs):
        return self.net.forward(inputs)

    def backward(self, preds, targets):
        loss = self.loss.loss(preds, targets)
        grad = self.loss.grad(preds, targets)
        grads = self.net.backward(grad)
        params = self.net.get_parameters()
        step = self.optimizer.compute_step(grads, params)
        return loss, step

    def apply_grad(self, grads):
        for grad, (param, _) in zip(grads, self.net.get_params_and_grads()):
            for k, v in param.items():
                param[k] += grad[k]
```

## æ•´ä½“ç»“æ„

æœ€åæˆ‘ä»¬å®ç°å‡ºæ¥æ ¸å¿ƒä»£ç éƒ¨åˆ†æ–‡ä»¶ç»“æ„å¦‚ä¸‹

```
tinynn
â”œâ”€â”€ core
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”œâ”€â”€ initializer.py
â”‚   â”œâ”€â”€ layers.py
â”‚   â”œâ”€â”€ losses.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ nn.py
â”‚   â””â”€â”€ optimizer.py
```
å…¶ä¸­ evaluator.py å’Œ initializer.py è¿™ä¸¤ä¸ªæ¨¡å—ç”±äºä¸æ˜¯æ ¸å¿ƒçš„æ¨¡å—ï¼Œä¸Šé¢æ²¡æœ‰å±•å¼€è®²ï¼Œè¿™é‡Œç®€å•è®²ä¸‹ä»–ä»¬çš„ä¸»è¦åŠŸèƒ½ã€‚evaluator.py ä¸»è¦å®ç°äº†ä¸€äº›å¸¸è§çš„è¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚å‡æ–¹å·®ã€é¢„æµ‹å‡†ç¡®ç‡ç­‰ç­‰ï¼‰ï¼Œinitializer.py ä¸»è¦å®ç°äº†å¸¸è§çš„å‚æ•°åˆå§‹åŒ–æ–¹æ³•ï¼ˆé›¶åˆå§‹åŒ–ã€Xavier åˆå§‹åŒ–ã€He åˆå§‹åŒ–ç­‰ï¼‰ï¼Œç”¨äºç»™ç½‘ç»œå±‚åˆå§‹åŒ–å‚æ•°ã€‚


## MNIST ä¾‹å­

æ¡†æ¶åŸºæœ¬æ­èµ·æ¥åï¼Œæˆ‘ä»¬æ‰¾ä¸€ä¸ªä¾‹å­æ¥ç”¨ tinynn è¿™ä¸ªæ¡†æ¶ run èµ·æ¥ã€‚è¿™ä¸ªä¾‹å­çš„åŸºæœ¬ä¸€äº›é…ç½®å¦‚ä¸‹

* æ•°æ®é›†ï¼šMNIST
* ä»»åŠ¡ç±»å‹ï¼šå¤šåˆ†ç±»
* ç½‘ç»œç»“æ„ï¼šä¸‰å±‚å…¨è¿æ¥ INPUT(784) -> FC(400) -> FC(100) -> OUTPUT(10)ï¼Œè¿™ä¸ªç½‘ç»œæ¥æ”¶ (N,784) çš„è¾“å…¥ï¼Œå…¶ä¸­ N æ˜¯æ¯æ¬¡è¾“å…¥çš„æ ·æœ¬æ•°ï¼Œ784 æ˜¯æ¯å¼  (28,28) çš„å›¾åƒå±•å¹³åçš„å‘é‡ï¼Œè¾“å‡ºç»´åº¦ä¸º (N,10) ï¼Œå…¶ä¸­ N æ˜¯æ ·æœ¬æ•°ï¼Œ10 æ˜¯å¯¹åº”å›¾ç‰‡åœ¨ 10 ä¸ªç±»åˆ«ä¸Šçš„æ¦‚ç‡
* æ¿€æ´»å‡½æ•°ï¼šReLU
* æŸå¤±å‡½æ•°ï¼šSoftmaxCrossEntropy
* optimizerï¼šAdam(lr=1e-3)
* batch_sizeï¼š128
* Num_epochsï¼š20
  
è¿™é‡Œæˆ‘ä»¬å¿½ç•¥æ•°æ®è½½å…¥ã€é¢„å¤„ç†ç­‰ä¸€äº›å‡†å¤‡ä»£ç ï¼ŒåªæŠŠæ ¸å¿ƒçš„ç½‘ç»œç»“æ„å®šä¹‰å’Œè®­ç»ƒçš„ä»£ç è´´å‡ºæ¥å¦‚ä¸‹

```
# example/mnist/run.py
net = Net([
  Dense(784, 400),
  ReLU(),
  Dense(400, 100),
  ReLU(),
  Dense(100, 10)
])
model = Model(net=net, loss=SoftmaxCrossEntropyLoss(), optimizer=Adam(lr=args.lr))

iterator = BatchIterator(batch_size=args.batch_size)
evaluator = AccEvaluator()
for epoch in range(num_ep):
    for batch in iterator(train_x, train_y):
      	# training
        pred = model.forward(batch.inputs)
        loss, grads = model.backward(pred, batch.targets)
        model.apply_grad(grads)
    # evaluate every epoch
    test_pred = model.forward(test_x)
    test_pred_idx = np.argmax(test_pred, axis=1)
    test_y_idx = np.asarray(test_y)
    res = evaluator.evaluate(test_pred_idx, test_y_idx)
    print(res)
```
è¿è¡Œç»“æœå¦‚ä¸‹

```
# tinynn
Epoch 0 	 {'total_num': 10000, 'hit_num': 9658, 'accuracy': 0.9658}
Epoch 1 	 {'total_num': 10000, 'hit_num': 9740, 'accuracy': 0.974}
Epoch 2 	 {'total_num': 10000, 'hit_num': 9783, 'accuracy': 0.9783}
Epoch 3 	 {'total_num': 10000, 'hit_num': 9799, 'accuracy': 0.9799}
Epoch 4 	 {'total_num': 10000, 'hit_num': 9805, 'accuracy': 0.9805}
Epoch 5 	 {'total_num': 10000, 'hit_num': 9826, 'accuracy': 0.9826}
Epoch 6 	 {'total_num': 10000, 'hit_num': 9823, 'accuracy': 0.9823}
Epoch 7 	 {'total_num': 10000, 'hit_num': 9819, 'accuracy': 0.9819}
Epoch 8 	 {'total_num': 10000, 'hit_num': 9820, 'accuracy': 0.982}
Epoch 9 	 {'total_num': 10000, 'hit_num': 9838, 'accuracy': 0.9838}
Epoch 10 	 {'total_num': 10000, 'hit_num': 9825, 'accuracy': 0.9825}
Epoch 11 	 {'total_num': 10000, 'hit_num': 9810, 'accuracy': 0.981}
Epoch 12 	 {'total_num': 10000, 'hit_num': 9845, 'accuracy': 0.9845}
Epoch 13 	 {'total_num': 10000, 'hit_num': 9845, 'accuracy': 0.9845}
Epoch 14 	 {'total_num': 10000, 'hit_num': 9835, 'accuracy': 0.9835}
Epoch 15 	 {'total_num': 10000, 'hit_num': 9817, 'accuracy': 0.9817}
Epoch 16 	 {'total_num': 10000, 'hit_num': 9815, 'accuracy': 0.9815}
Epoch 17 	 {'total_num': 10000, 'hit_num': 9835, 'accuracy': 0.9835}
Epoch 18 	 {'total_num': 10000, 'hit_num': 9826, 'accuracy': 0.9826}
Epoch 19 	 {'total_num': 10000, 'hit_num': 9819, 'accuracy': 0.9819}
```
å¯ä»¥çœ‹åˆ°æµ‹è¯•é›† accuracy éšç€è®­ç»ƒè¿›è¡Œåœ¨æ…¢æ…¢æå‡ï¼Œè¿™è¯´æ˜æ•°æ®åœ¨æ¡†æ¶ä¸­ç¡®å®æŒ‰ç…§æ­£ç¡®çš„æ–¹å¼è¿›è¡ŒæµåŠ¨å’Œè®¡ç®—ï¼Œå‚æ•°å¾—åˆ°æ­£ç¡®çš„æ›´æ–°ã€‚ä¸ºäº†å¯¹æ¯”ä¸‹æ•ˆæœï¼Œæˆ‘ç”¨ Tensorflow 1.13 å®ç°äº†ç›¸åŒçš„ç½‘ç»œç»“æ„ã€é‡‡ç”¨ç›¸åŒçš„é‡‡æ•°åˆå§‹åŒ–æ–¹æ³•ã€ä¼˜åŒ–å™¨é…ç½®ç­‰ç­‰ï¼Œå¾—åˆ°çš„ç»“æœå¦‚ä¸‹

```
# Tensorflow 1.13.1
Epoch 0 	 {'total_num': 10000, 'hit_num': 9591, 'accuracy': 0.9591}
Epoch 1 	 {'total_num': 10000, 'hit_num': 9734, 'accuracy': 0.9734}
Epoch 2 	 {'total_num': 10000, 'hit_num': 9706, 'accuracy': 0.9706}
Epoch 3 	 {'total_num': 10000, 'hit_num': 9756, 'accuracy': 0.9756}
Epoch 4 	 {'total_num': 10000, 'hit_num': 9722, 'accuracy': 0.9722}
Epoch 5 	 {'total_num': 10000, 'hit_num': 9772, 'accuracy': 0.9772}
Epoch 6 	 {'total_num': 10000, 'hit_num': 9774, 'accuracy': 0.9774}
Epoch 7 	 {'total_num': 10000, 'hit_num': 9789, 'accuracy': 0.9789}
Epoch 8 	 {'total_num': 10000, 'hit_num': 9766, 'accuracy': 0.9766}
Epoch 9 	 {'total_num': 10000, 'hit_num': 9763, 'accuracy': 0.9763}
Epoch 10 	 {'total_num': 10000, 'hit_num': 9791, 'accuracy': 0.9791}
Epoch 11 	 {'total_num': 10000, 'hit_num': 9773, 'accuracy': 0.9773}
Epoch 12 	 {'total_num': 10000, 'hit_num': 9804, 'accuracy': 0.9804}
Epoch 13 	 {'total_num': 10000, 'hit_num': 9782, 'accuracy': 0.9782}
Epoch 14 	 {'total_num': 10000, 'hit_num': 9800, 'accuracy': 0.98}
Epoch 15 	 {'total_num': 10000, 'hit_num': 9837, 'accuracy': 0.9837}
Epoch 16 	 {'total_num': 10000, 'hit_num': 9811, 'accuracy': 0.9811}
Epoch 17 	 {'total_num': 10000, 'hit_num': 9793, 'accuracy': 0.9793}
Epoch 18 	 {'total_num': 10000, 'hit_num': 9818, 'accuracy': 0.9818}
Epoch 19 	 {'total_num': 10000, 'hit_num': 9811, 'accuracy': 0.9811}
```
![](media1.jpg)
å¯ä»¥çœ‹åˆ°ä¸¤è€…æ•ˆæœä¸Šå¤§å·®ä¸å·®ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡éƒ½æ”¶æ•›åˆ° 0.982 å·¦å³ï¼Œå°±å•æ¬¡çš„å®éªŒçœ‹æ¯” Tensorflow ç¨å¾®å¥½ä¸€ç‚¹ç‚¹ã€‚
# ç»“è¯­
åœ¨è®¾è®¡å’Œå®ç° tinynn çš„è¿‡ç¨‹ä¸­ä¸ªäººç¡®å®å­¦ä¹ åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼ŒåŒ…æ‹¬å¦‚ä½•æŠ½è±¡ã€å¦‚ä½•è®¾è®¡ç»„ä»¶æ¥å£ã€å¦‚ä½•æ›´æ•ˆç‡çš„å®ç°ã€ç®—æ³•çš„å…·ä½“ç»†èŠ‚ç­‰ç­‰ã€‚æ·±åº¦å­¦ä¹ æˆ–æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¡†æ¶æ¶µç›–å…·æœ‰è®¸å¤šéšè—å±‚çš„å„ç§ç¥ç»ç½‘ç»œæ‹“æ‰‘ï¼ŒåŒ…æ‹¬æ¨¡å¼è¯†åˆ«çš„å¤šæ­¥éª¤è¿‡ç¨‹ã€‚ç½‘ç»œä¸­çš„å±‚è¶Šå¤šï¼Œå¯ä»¥æå–ç”¨äºèšç±»å’Œåˆ†ç±»çš„ç‰¹å¾è¶Šå¤æ‚ã€‚å¸¸è§çš„Caffeï¼ŒCNTKï¼ŒDeepLearning4jï¼ŒKerasï¼ŒMXNetå’ŒTensorFlowç­‰éƒ½æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚æ·±åº¦å­¦ä¹ ï¼Œä½œä¸ºç›®å‰æœ€çƒ­çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä½†å¹¶ä¸æ„å‘³ç€æ˜¯æœºå™¨å­¦ä¹ çš„ç»ˆç‚¹ã€‚ 
